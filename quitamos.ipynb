{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Read the Excel file\n",
    "# name = 'Data_INC'\n",
    "name = 'Data_INC' # problem, problem_task, CRE_INC\n",
    "\n",
    "file_path = os.path.join(raw_data, f\"{name}.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "df= df.drop(columns=\"Affected User\")\n",
    "\n",
    "# Step 2: Determine split size\n",
    "# Assuming each row is approximately 1 KB, calculate rows per file for 100 MB limit\n",
    "approx_row_size_kb = 1  # Adjust based on your data\n",
    "rows_per_file = (24 * 1024) // approx_row_size_kb\n",
    "\n",
    "# Step 3: Split the DataFrame\n",
    "num_files = (len(df) // rows_per_file) + 1\n",
    "print(num_files)\n",
    "\n",
    "# Create a directory to store smaller files\n",
    "output_dir = f'{name}_split_files'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 4: Save smaller files in Parquet format\n",
    "for i in range(num_files):\n",
    "    start_row = i * rows_per_file\n",
    "    end_row = min((i + 1) * rows_per_file, len(df))\n",
    "    df_split = df.iloc[start_row:end_row]\n",
    "    output_file = os.path.join(output_dir, f'{name}_split_file_{i+1}.parquet')\n",
    "    try:\n",
    "        df_split.to_parquet(output_file, engine='fastparquet', compression='gzip')\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file {output_file}: {e}\")\n",
    "\n",
    "print(f'Successfully split into {num_files} files.')\n",
    "\n",
    "# # Step 5: Merge files (when needed)\n",
    "# # Example of merging back the files\n",
    "# merged_df = pd.DataFrame()\n",
    "# for i in range(num_files):\n",
    "#     input_file = os.path.join(output_dir, f'{}split_file_{i+1}.parquet')\n",
    "#     df_part = pd.read_parquet(input_file)\n",
    "#     merged_df = pd.concat([merged_df, df_part], ignore_index=True)\n",
    "\n",
    "# print('Successfully merged files back into a single DataFrame.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc06436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57097b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd6962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4323c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52a136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cff6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd5011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae7855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. CARGAR DICCIONARIO DE MAPEOS (Equipo -> Grupo)\n",
    "# ============================================================\n",
    "\n",
    "df_grupos_1lvl = pd.read_excel(\n",
    "    os.path.join(raw_data, \"grupos_audit.xlsx\"), sheet_name=\"1LVL\", dtype=str\n",
    ")\n",
    "df_grupos_IT = pd.read_excel(\n",
    "    os.path.join(raw_data, \"grupos_audit.xlsx\"), sheet_name=\"IT_TEAMS\", dtype=str\n",
    ")\n",
    "\n",
    "df_grupos = pd.concat([df_grupos_1lvl, df_grupos_IT], ignore_index=True)\n",
    "df_grupos[[\"Equipo\", \"Grupo\"]] = df_grupos[[\"Equipo\", \"Grupo\"]].map(clean_str_compact)\n",
    "\n",
    "map_dict = df_grupos.set_index(\"Equipo\")[\"Grupo\"].to_dict()\n",
    "\n",
    "# ============================================================\n",
    "# 2. CARGAR Y PREPARAR INCIDENCIAS\n",
    "# ============================================================\n",
    "\n",
    "load = True\n",
    "try:\n",
    "    log_separator(\"Data_INC.xlsx Source -> ServiceNow\", level=2)\n",
    "    logger.info(\"Loading and processing Data_INC.xlsx\")\n",
    "\n",
    "    # Ruta al pickle y fallback a Excel si no existe\n",
    "    pickle_path = os.path.join(raw_data, \"init_Data_INC.pkl\")\n",
    "    if os.path.exists(pickle_path) and load:\n",
    "        logger.info(f\"Loading Data_INC from pickle: {pickle_path}\")\n",
    "        df = pd.read_pickle(pickle_path)\n",
    "    else:\n",
    "        excel_path = os.path.join(raw_data, \"Data_INC.xlsx\")\n",
    "        logger.info(f\"Loading Data_INC from Excel: {excel_path}\")\n",
    "        df = pd.read_excel(excel_path)\n",
    "        df.to_pickle(pickle_path)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading Data_INC: {e}\")\n",
    "\n",
    "df = pd.read_pickle(os.path.join(raw_data, \"init_Data_INC.pkl\"))\n",
    "df_origen = df.copy()\n",
    "\n",
    "\n",
    "def twk_df(df):\n",
    "    return (\n",
    "        df.pipe(clean_headers)\n",
    "        .pipe(\n",
    "            set_df_column_order,\n",
    "            [\n",
    "                \"NUMBER\",\n",
    "                \"PROBLEM\",\n",
    "                \"STATE\",\n",
    "                \"LAST_ASSIGNMENT_GROUP\",\n",
    "                \"ASSIGNMENT_GROUP\",\n",
    "                \"CREATOR_GROUP\",\n",
    "            ],\n",
    "        )\n",
    "        .loc[\n",
    "            lambda x: (x['CREATED'].dt.year > 2022)\n",
    "            # & (x['CREATED'] < pd.to_datetime('2025-09-01'))\n",
    "        ]\n",
    "        .pipe(sort_values_of_incidents)\n",
    "    )\n",
    "\n",
    "print(df.shape, \"lectura\")\n",
    "df = twk_df(df)\n",
    "print(df.shape , \"filtros previos\")\n",
    "\n",
    "\n",
    "# Metadatos por grupo (agrupado por NUMBER)\n",
    "df[\"_ORDER\"] = df.groupby(\"NUMBER\").cumcount()\n",
    "df[\"_SIZE\"] = df.groupby(\"NUMBER\")[\"NUMBER\"].transform(\"size\")\n",
    "df[\"_LAST\"] = df[\"_ORDER\"] == (df[\"_SIZE\"] - 1)\n",
    "\n",
    "# ============================================================\n",
    "# 3. LIMPIAR Y MAPEAR COLUMNAS DE SERVICENOW\n",
    "# ============================================================\n",
    "\n",
    "group_cols = [\"LAST_ASSIGNMENT_GROUP\", \"ASSIGNMENT_GROUP\", \"CREATOR_GROUP\"]\n",
    "group_in_dict = [\"_Grupo_LAG\", \"_Grupo_ASG\", \"_Grupo_CRG\"]\n",
    "\n",
    "# Limpieza de strings\n",
    "df[group_cols] = df[group_cols].map(clean_str_compact)\n",
    "casos_audit[\"INC_Company_null\"] = df[df['COMPANY'].isnull()]\n",
    "casos_audit_descrip[\"INC_Company_null\"] = \"Casos sin empresa 'vacio'\"\n",
    "\n",
    "df['COMPANY'] = df['COMPANY'].fillna(\"_VACIO_\").apply(standardize_company_name)\n",
    "\n",
    "to_AZS = [\"ALLIANZSEGUROS SA\", \"ALLIANZ SEGUROS SA\", \"ALLIANZ COMPANIA DE SEGUROS Y REASEGUROS SA\"]\n",
    "df['COMPANY'] = df['COMPANY'].replace(to_AZS, 'ALLIANZ SEGUROS SA')\n",
    "keep = [\"ALLIANZ SEGUROS SA\",\"ALLIANZ SPAIN EXTERNAL\",\"ALLIANZ TECHNOLOGY SL\",\"BBVA\"]\n",
    "\n",
    "# Mapear con fallback (\"\" si no existe en el dict)\n",
    "df[group_in_dict] = df[group_cols].map(lambda x: map_dict.get(x, \"\"))\n",
    "\n",
    "# Flags de grupo\n",
    "df[\"_Grupo_RES_any\"] = (df[group_in_dict] != \"\").any(axis=1)\n",
    "df[\"_Grupo_RES_all\"] = (df[group_in_dict] != \"\").all(axis=1)\n",
    "\n",
    "df[\"_Grupo_No1LVL_any\"] = (df[group_in_dict] != \"1LVL-OTROS\").any(axis=1)\n",
    "df[\"_Grupo_No1LVL_all\"] = (df[group_in_dict] != \"1LVL-OTROS\").all(axis=1)\n",
    "\n",
    "df[\"_Grupo_GLOB_any\"] = (df[group_in_dict] == \"GLOB-OTROS\").any(axis=1)\n",
    "df[\"_Grupo_GLOB_all\"] = (df[group_in_dict] == \"GLOB-OTROS\").all(axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# 4. REGLAS DE ÚLTIMO VALOR\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    # A) Últimos valores de grupos\n",
    "    last_values = df.groupby(\"NUMBER\")[group_cols].last()\n",
    "    last_mapped = last_values.map(lambda x: x in map_dict).add_prefix(\"_IS_Last_\")\n",
    "    df = df.merge(last_mapped, on=\"NUMBER\", how=\"left\")\n",
    "\n",
    "    # B) Último estado/causa\n",
    "    last_values = df.groupby(\"NUMBER\")[[\"STATE\", \"CAUSE_CODE\"]].last()\n",
    "    df[\"_LAST_STATE_CLOSED\"] = df[\"NUMBER\"].map(last_values[\"STATE\"] != \"CLOSED\")\n",
    "    df[\"_LAST_CAUSE_CODE_WaD\"] = df[\"NUMBER\"].map(\n",
    "        last_values[\"CAUSE_CODE\"] != \"Works as designed\"\n",
    "    )\n",
    "    df[\"_LAST_CAUSE_CODE_Cancelled\"] = df[\"NUMBER\"].map(\n",
    "        last_values[\"CAUSE_CODE\"] != \"Cancelled\"\n",
    "    )\n",
    "    df[\"_WaD_Closed\"] = df[\"_LAST_STATE_CLOSED\"] & df[\"_LAST_CAUSE_CODE_WaD\"]\n",
    "    df[\"_Cancelled_Closed\"] = (\n",
    "        df[\"_LAST_STATE_CLOSED\"] & df[\"_LAST_CAUSE_CODE_Cancelled\"]\n",
    "    )\n",
    "\n",
    "    # C) Últimos GLOB/Prioridad\n",
    "    last_values = df.groupby(\"NUMBER\")[[\"_Grupo_GLOB_all\", \"CURRENT_PRIORITY\"]].last()\n",
    "    mask_excluir = last_values[\"_Grupo_GLOB_all\"] & last_values[\n",
    "        \"CURRENT_PRIORITY\"\n",
    "    ].isin([\"P4\", \"P3\"])\n",
    "    df[\"_Glob_P4\"] = df[\"NUMBER\"].map(~mask_excluir)\n",
    "\n",
    "    # D)\n",
    "    # last_values = df.groupby(\"NUMBER\")[[\"STATE\", \"CAUSE_CODE\"]].last()\n",
    "    # df[\"_LAST_STATE_CLOSED\"] = df[\"NUMBER\"].map(last_values[\"STATE\"] != \"CLOSED\")\n",
    "    # df[\"_LAST_CAUSE_CODE_WaD\"] = df[\"NUMBER\"].map(\n",
    "    #     last_values[\"CAUSE_CODE\"] != \"Works as designed\"º\n",
    "    # )\n",
    "    # df[\"_WaD_Closed\"] = df[\"_LAST_STATE_CLOSED\"] & df[\"_LAST_CAUSE_CODE_WaD\"]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Flags agregados de último\n",
    "prefix = \"_IS_Last_\"\n",
    "df[\"_Grupo_LAST_any\"] = df.filter(like=prefix).any(axis=1)\n",
    "df[\"_Grupo_LAST_all\"] = df.filter(like=prefix).all(axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# 5. FILTROS SECUENCIALES\n",
    "# ============================================================\n",
    "\n",
    "# Define masks based on conditions\n",
    "df['_IS_Parent'] = df['PARENT_INCIDENT'].fillna('') == '' # remove childs\n",
    "df['_IS_FCR'] = df['FCR'] != True\n",
    "df['_IS_COMPANY'] = df['COMPANY'].isin(keep)\n",
    "\n",
    "filters = [\n",
    "    \"_Grupo_RES_all\",\n",
    "    \"_Grupo_No1LVL_all\",\n",
    "    \"_WaD_Closed\",\n",
    "    \"_Cancelled_Closed\",\n",
    "    \"_Glob_P4\",\n",
    "    \"_IS_Parent\",\n",
    "    \"_IS_FCR\",\n",
    "    \"_IS_COMPANY\"\n",
    "]\n",
    "df = apply_filters(df, filters)\n",
    "\n",
    "# ============================================================\n",
    "# 6. ORDEN FINAL\n",
    "# ============================================================\n",
    "\n",
    "df_incidents = df.pipe(sort_values_of_incidents, \"_SIZE\")\n",
    "\n",
    "try:\n",
    "    del df, df_grupos, df_grupos_IT, df_grupos_1lvl\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "incident_tickets = df_incidents['NUMBER'].unique()\n",
    "problem_tickets = df_incidents[\"PROBLEM\"].unique()\n",
    "\n",
    "# cancelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cebd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37560c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cded051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3a440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3dbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f66b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
