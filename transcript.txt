Tienen duplicados o usan la misma información para no emigrarlos dos veces o optimizarlos en un único código todos estos casos de uso, pues que usan información muy similar entonces con esto ya estamos validando no que las áreas bueno porque áreas ya están validando que no estén duplicando esfuerzos ni procesos que hacen la lo mismo y además encima de todo lo que las áreas pueden migrar a la ADP tenemos IDMCo que al final nos permitirá trazar un poco pues sus casos de uso no y poder saber si en algún momento están calculando o dan de alta el mismo KPI con con cálculos distintos o cosas similares no sé si tu Workshops con con las áreas, pues se podrá realizar este este cálculo de de calidad, no pero. Previo a eso no sé si si aparte de bueno las mismas áreas no son las que establecen y detectan qué es lo que se va a llevar, y qué es lo que no. Eh se ha hecho un Challenge Ch esto digamos que se te se tienen ya identificadas todas las tablas de De esas, cómo cómo ha sido un poco más ese proceso o si nos puedes aterrizar. Con las que al final las áreas procesgada o algún otro tipo de anis que hagan aprender desde las áreas, pero no están generando tablas al uso de de esas relaciones consumen esta información en sus procesos y los que ellos tienen que emigrar son sus procesos no que leen el origen de datos o el Warhouse y tienen sus Outputs y en vez de hacerlo en Sa o sea está migración es lo que har ese de esa información más hacia adelante, es donde las lo bueno las áreas ya no, ya no ejecutan ciertos procesos, van a no solo solo migran ciertos procesos. Pero entonces desde raíz y desde si p que hay una se pasan todas las tablas, no porque es un copia y pega al final. Y a futuro, yo supongo que se hará hacia atrás, se recorrido de bueno si ciertos procesos, ya no se ejecutan ciertas tablas, ya no se están tocando. Se tendrá que que quitar no de por complementar técnica y en la migración técnica teníamos varios objetivos. La primera es no ser un cuello de botella para negocio por eso empezamos haciéndo el espejo S y mientras tanto hacemos el Wall Strep dos, mientras las áreas van emigrando que el Westtren dos es precisamente rélicar todo lo que a día de de hoy hacer motor hacerlo de forma nativa en ADP, pero aquí el objetivo no es ni optimizar el modelo de datos ni cambiar la la la forma en que organizaizamos los datos es mantener la misma estructura, pero sin independencia técnica de de esas vale motivos por los que esto se hace así primero porque apremia el tiempo8 está a la vuelta de la esquina y y no tenemos tiempo de hacer una reingeniería a grande y y y migrar entonces empezamos haciendo la migración técnica, pero aparte es que el propio Sas a día de hoy tenemos limitaciones para para poder entender cómo se explotan las salas porque no tenemos Locks como tal de de explotación entonces realmente o sea confiamos un poco en la en en el feedback que nos dan los usuarios más las incidencias que hay no que muchas veces no sabemos si una tabla se usa o no hasta que la quitas si alguien se queja pero si si no es así no no tenemos un lo como tal entonces todas estas limitaciones hacen imposible a día de hoy plantear una reingenería entonces empezamos eliminándola la la la dependencia técnica de Sas y a partir de aquí empezamos a construir sí que en paralla hemos hecho gasto Merctric para testear cuan factible es hacerlo hemos visto que sí y en caso Merctric sí que hemos remodelado desde cero pero pero para hacerlo al resto de dominios, habrá que hacerlo una vez no hemos quitado de de encima la dependencia técnica. De hecho este año que viene vamos a empezar a hacer este ejercicio pero será en paralelo a la migración técnica de S Igualmente si nos pueden actualizar creo que el día que tuvimos la primera todas las tablas que había en 2022 en el SS. Y cuál es de esas tablas usaban desde las áreas es totalmente servido, pues para tener la documentación pero en ese listado todas las tablas estaban marcadas como que las utilizaban desde las áreas entonces presuntamente todas las tablas que hay en utilizan por alguna de las áreas de eso que comentamos aquí no tenemos otra manera de contarlo que no sea confiando en el usuario y el usuario ya sabemos que muchas veces va a ir a máximo dirá no no mantenemos todas las tablas no sea que se rompa pero no sabemos el uso efectivo de esas tablas ni podemos saberlo porque en esta es limitación una vez estemos en en ADP ahí empezaremos a a ver si realmente qué se consume y cómo Management el nuevo Roadmap que que se tenía actualmente creo que fue el mismo 2 de octubre Por si tienen esa esa presentación y si también pueden Teníamos un borra borra no sé si se haya cambiado algo la que presentaste ese díatectura que se envía a grupo aquí ya bueno ya tenemos el el asessment no que evalúa el el estatus de cada una de las Cabili que hay eh, no sé si si si podemos digamos que verlo In situ con ustedes ir seleccionando algunas de las De los requerimientos que se piden y cómo cómo han han bueno para sustentar sustentar un poco el este cortatus. E Que en algunos bueno, no se tienen nada hecho, por ejemplo el Model mapping y en algunos bueno ya se tiene un curso estatus de nivel dos. Eh, nivel uno, etc o si prefieren nosotros seleccionamos y las mandamos a ustedes y así ya también pueden poner a punto subir quizá evidencias Aunque entiendo que esto lo lo verán en noviembre, no con con global arquitecto bueno que estamos en parte de ir viendo operativamente nos compartes pantalla operativamente ver cómo está funcionando todo el tema de arquitectura no en vivo se os parece factible no simamos viendo las cómo está este estructura arquitectura, la plataforma, todo lo que nos ha funcionado ya es que tiene si os parece todo un barrido de esta forma y así nos enteramos nosotros y también podemos ir binqueando no en la práctica no sé os parece bien así si hubi grupo lo completó David entonces quizás estaría bien involucrarlo a a él para esa parte y y y que estos requisitos que quieran validar los compañeros de auditoría, pues lo él puede proporcionar las las evidencias o bueno lo ves con Isaac, pero alineándolo con con él vale vale vale eso con la con la con conocer qué qué conexiones son las que están este esas esas fuentes de datossource, no que sería EP y quizá otras eh podríamos empezar por ahí empezar desde allí cuáles son las que están actualmente conectadas a día de hoy de entrada de ficheros al Warhouse del ADP tendríamos lo que es EPAC y como el primer caso de uso que sea abordado ha sido el de Castomercentric, pues tenemos bastantes inputs son de fuentes de Castomer en este caso son si es queéis las digo son las de Digital Lis las de Obe Campain y luego hay una parte que retroalimenta de procesos que son de analytics Y en los propios Data no habilitados para cada uno de las unidades de negocio o es lo que no entiendo o o vienen desde unsource hasta el. para dar la información y si al final se tienen cargar o son datos globales que entrarán al Warehouse corporativo, pues se habilitaron espacio adicional para que se tejen estos ficheros y y el inicio de proceso de esta solicitud de conexión supongo que va por la vía de Wave no ahí está ahí se aprovecha ese ese canal para recoger estas casos de uso en la creo que en la en el preform no hay habilitada una pregunta. Sí, bueno aquí habrá porque ahora habrá supongo que ahora hecho solo el de clientes pero bueno tiene que ahí el dominio de póliza el dominio de siniestro y demás que seguramente cuando empecemos a abordar estos nuevos dominios nos encontraremos, pues que necesitamos que o a día de hoy hay proveedores que me han información al o sea tenemos que preparar la plataforma para que admita que estos proveedores nos envíen esta información y luego abra los casos de uso de áreas como podrían ser operaciones, no pues que tienen el el voicebot o o el Chatbot al final es información que solo explotan ellos que no es una información que se vaya a explotar a nivel de toda la compañía. Entonces estos datos los Ellos cuando hagan su pues si quedarán en el preforme y dirán necesitamos estos que se ha este espacio necesitamos introducir información de estos proveedores o de lo que necesiten para que se prepare la plataforma que ya se tienen todas los dominios no y ya se tienen estos espacios eh se se paga hasta que se usa hasta que se haga la conexión o ya o o digamos que que ya hay una facturación de. Aunque estén vacíos. Quizá de de estos espacios no no lo sé € que empiezas a meter y empieza a crecer el volumen, pero es cuando se empieza a a pagar más de los de los puntos pendientes principalmente porque hasta que tampoco hemos democratizado a ADP no tenía mucho sentido entender como cómo repartir los costes, pero sí la idea a futuro es es hacerlo así lo que pasa es que no es tan sencillo o sea tenemos que hacer un proyecto para hacerlo porque a día de hoy lo que tenemos en de visibilidad en el monitor de costes es la la visión por por componente, pero no por caso de uso entonces hacer esta alocación a casos de uso de del gasto en en componentes pues bueno no no es no es trivial hay que hay que ver la manera de de hacerlo vale decir ahora por ejemplo puedes tener 1 a en concreto que consume recursos del psicpour y de y del Data Leke, pero no sabemos por cada proyecto que consumo hace no eso es lo que lo que queremos llegar a a entender sobre todo en proyectos Cross, pero después que esta Sli que estáis mostrando es una Slipe de la arquitectura con visión componentes, pero no es la la arquitectura como tal que hemos definido creo que os pasamos otra en el en el DSB pero yo os la os la pongo no sé es que tengo esta pero la la versión en pantalla más completa no sé si tú la tienes delante pero es la que mostrábamos en el DSD de con los diferentes entornos como tres capas dentro de lo que es el Enterprise de Tower House Enterprise de Towerhouse abarca hasta las capas de consumo, es decir, hasta las actuales librerías SS, que los usuarios consumen a partir de ahí le llamamos vles Dacars a todo lo que los usuarios hagan por encima sea de para procesar o para almacenar entonces en estas tres capas hay tres entornos que abarcan cada cada una de las tres capas vale en un entorno de desarrollo pero producción y producción y esto es el terreno de ejecución de alias technologes, es decir, quien mueve los datos por desde la fuente transaccional hasta la capa de consumo es technology y lo hace con diferentes tecnologías con el lago de datos o con con el siqupurs esto no cambia mucho de cómo estaba a día de hoy quien tiene acceso para crear tablas en las librerías oficiales es es también aliance technologe entonces por cada uno de de estos entornos, pues hay una serie de tecnologías que mueven estos datos como normas general en el en el lago de datos no va a acceder el usuario final, porque aquí los datos entran en crudo y la la visión o la diferencia respecto de ahora es que lo los datos aquí ya entraban como preprocesados y y limpiados aquí la idea es que todo entre en bruto y toda la transformación y lógica de negocio se haga en la en las capas posteriores vale entonces el usuario donde tiene acceso a consumo y a partir de consumo puede hacer procesos para esos procesos tiene varias herramientas o sea por un lado tiene los los clusters deKS Tiene una una pequeña parcela en el Lago de datos y tiene una parcela en el Ppool y y los clusters de de Spark entonces con todo eso ellos se pueden montar procesos y esos procesos deberían ir en el caso de un Dashboard por ejemplo automatizado, desarrollarían en preproducción y luego la creta producción pasaría por nuestro por nuestro stage o sea por por por parte de analítis y a partir de que está en producción, pues se alimentará con la regularidad que sea para consumirlo vía Power BI o en el caso de Machillerring y tal pues tenemos otros otros procesos y entiendo que en la bueno cuando se desarrolla cuando un equipo de negocio hace un desarrollo este se hace en local, no luego para llevarlo a preproducción y luego a producción, ahí es donde ustedes entran y digamos que tienen automatizados o se buscaría automatizar no este flujo de. De quizá Tts de usuario, Test técnico, no de que el código está bien hecho para para así subirlo a producción esto no sé si lo tienen. Ya digamos que aterrizadoproducción tienen libertad para para desarrollar procesos. El tema es para pasar a producción sí que hay una serie de de de temas técnicos que se que se chequea y ahora todo todo esto lo estamos industrializando con con la herramienta de dataportal, que seguramente Alejandro ya ya ha enseñado Isaac creo quemas de de cómo se hacía y los puedo buscar he pasado hoy Light grande en lo que este proceso más técnico que lo lleva tecnología, es donde entran los Data Contrats no eh seguramente irá también vinculado a un web no porque si pones un cambio de modelo de datos yo creo que irá con Waze y entonces allí entra, pues la definición de los KPIs nuevos que se van a a calcular ambas cosas para crear crudo desde el sururce. Para que les llegue a ellos ya en sus capas de consumo y también para corregir errores supongo no o sea esos si como las dos este los dos usos de un data contract seis ni 12 y cosas así también es para mejorar la calidad del dato correcto tablas antes de llegar al al Datawer House tiene unos controles mínimos de de calidad y ayudar al coordinación entre equipos internos de technology, pero de cara al usuario, el Datacontrack no es transparente para el para el usuario el usuario lo que va a ver es con IDMC poder hacer reglas de calidad y a partir de esas reglas de calidad puede levantar tickets y esos tickets deberían retro o sea deberían modificar los procesos mejorarlos y y por lo tanto la la monitorización del data Quality se debería haber una evolución Quality Fbour de Ata como funcionan y tal y ya están corriendo en ADP con cual si queréis ver alguno y tal podríamos hablar con la gente de SG que sonla la consultora que hecho el comer Centric y lo vemos con ellos pero sí ataicamente es un es un si no recuerdo mal aquí ya me corí si no y explica a cada campo, pues una inscripción del campo qué tipo de formato es y luego también las reglas de calidad que se le aplican a largo de todas las capas del ADP. cuestiones de calidad y también de formato Res ahora mismo Tecchnology también es la que se encarga porcionar de todo este flujo de pasar desde la ingesta hasta consumo, etc. no aquí entiendo que ya con lo que habéis dicho ya incidencial a los sto entiendo porque por ejemplo no sabes algo no sabe eso no conomiten se puede seguir la carga y errores que sean críticos, no pues yo qué sé que me llegue el DNI frío pero no se puede seguir la carga porque si no al final no sabrás el cliente que hay detrás entonces estas cosas se podrá levantar en el momento de empezar a hacer la carga. detenidos o otro tipo de personas no sé el detaúner que al final es el responsable de este dato aquí hay cuatro4 microfases vale es decir está no persistetayinaria rodatavolta vale entonces estas áreas la todas estas stages los datos se mueven de una cabla a otra con transformaciones entonces en esas transformaciones hay se ejecuta un data contra y ese data contract genera una traza de calidad del dato que vuelca en un Dashboard de de PowerBI vale ese Datgeboard es el que el área usuaria va a tener disponible. Claro esto solo es para nuevo remodelada datos. Es decir a día de hoy solo está disponible para Castomerceri en la medida en la que ampliemos dominios. Esto es avanzaría más dominios entonces ese sería como el Dashboard de calidad técnica con cosas muy básicas, no de oye número de por ejemplo ahora qué qué es lo que vemos número dedo de esto nos ha permitido duplicamediación de limpieza de estos duplicados en el en el transaccional o Missing valius en emails o temas así no todo esto es lo que podrías tener en esos Des porque claro eso es solo para las nuevas modelos de datos si el área usuaria además quiere aplicar reglas expertas tiene IreneC o sea reglas de calidad tiene ILMC en los nuevo en los antiguos dominios de datos y todo lo que es la réplica de SAS pueden ser también con IDMC sus propias reglas entonces en la propia herramienta de IDMC se pueden levantar piicks de calidad del dato y esos son los tickets que después deberían llevar al al área generar planes de remediación que levantarían a su vez, pues tickets en gira y y de demandas o bien por Minor Diman o bien procesos más o sea proyectos más importantes no de desarrollo para subsanar los los problemas que haya aquí es difícil poner un proceso único porque habrá cosas de y cosas que sean muy importantes no por ejemplo el duplicado de clientes no es nada trivial de hecho ahora el problema que hay es que el el duplicado de clientes detectado pues en el transaccional no se puede limpiar del todo porque entra en conflicto con la lógica de negocio no o sea por qué se duplica clientes pues porque si un cliente va a un broker y va a un a un agente y tiene datos de contacto diferentes pues se tiene que mantener ambos datos de de contactos no entonces por lógica se tendría que repensar muy bien no como como defmos un cliente en Epack para que esto después pues vuelque bien en en el Data Warehouse obviamente hay otras medidas para limpiarlo a lo que estamos haciendo es el Data Warehouse detectar que clientes son duplicados y mostrar una visualización sin eso duplicados, pero lo ideal no lo lo que dice la teoría de Data Quality es que debes. Limpiar los datos cuanto más en origen mejor pero bueno como digo como es muy complicado poner una regla Que que defina todos los procesos de data Quality, lo que de momento tenemos es elC una una herramienta entre comillas de de levantamiento de incidencias para inventariarlas y a partir de aquí es el propio usuario el Datauner, el el encargado no de de levantar estos problemas a en en la demanda de IT para mejorarlos OK de la forma de avisarnos sería por directamente a través delmestre o alguna alerta todo caso sí, vamos a tener o en toda la DP aquí lo que se puede hacer es que cada vez que tú haces una regla, tú puedes decir que es el ejecute memento una vez al día y que si esa vez al día da ciertos errores que te avisa entonces directamente una persona que llegue a alguien que ya implementa reglas de calidad, podrá tener notificación de forma pasiva si algún día falla algo vale entonces no solo es conocer la calidad del día que tú lo ejecutas sino que puedes hacer un squedull para que se vaya ejecutando de forma constante y que el día que pete al directamente te avise para que seas consciente del error y empieces a gestionar la calidad de una métrica cae por debajo del umbral, pues recibirías alerta pero también si yo por ejemplo soy usuario de tablas en las que Danny es Souler y detecto un problema de calidad ahí puedo abrirle un ticket a Dani y decir oye mira es que he mirado este campo y y no tiene nada que ver con lo que sería entonces aquí hay un problema y quizá Dani pues como tiene 80 tablas no se ha mirado en concreto de esa tabla de ese campo pero yo como usuario como como entro en las en sus tablas y y lo veo pues puedo levantarle los tickets y lo tendría también en en en IBMG Pues allí tendríamos también en el Dataportal un canal de incidencias generales, donde si tú te suscribes como usuario técnico esto lo explicaremos en las jornadas de por Iit no sé si vais a ir pero se explicará no el la idea es y bueno vamos a hacer un un ciclo de formación y tal pero la idea es que cualquier Per que sea ADP Reciía esas notificaciones técnicas de incidencias generales proceso de DVT y lo que quedaría creo que bueno ya no sé explicaron que están en conversaciones con grupo para intentar hacer la conexión a lo que ya son ya lo que sería el linaje que que que viene en la parte de Power BI. Creo que no actualmente no no se no se permite no hacer una conexión eh eso bueno supongo que hasta que grupo lo lo solucione o se cambie no del modelo de Paul Vide, creo que nos comentaron que. Que que se va a cambiar o se va a transformar un poco el entorno, ahí es donde ya se podrá hacer la conexión para ya cerrar todo el ciclo, no desde que viene desde el surce hasta hasta lo que ya se consume, pero ya directamente desde desde los Dashbots, no. No sé si nos estamos dejando algo entre medias, porque bueno al final técnicamente bueno no sé si hay algún otro flujo importante donde Donde se tenga que intervenir quizá con con validaciones de calidad incidencias o sea por porque como tenemos que explotar acceder a al en el tenal de Power BI a los metatos, tal y como está estructurado PowerBI, accederíamos a los metadatos de cualquier OE no solo de la nuestra entonces ahí abre un potencial problema de seguridad si si DMC accede a a este tema entonces eso es lo que por lo que el grupo a día de hoy no nos nos impide conectar el segu lo que tenemos que analizar es si en la evolución de A Fabri que será la la siguiente el siguiente paso no en la plataforma claro allí el ten ante único es decir PowerB es como si se fusionara con con toda la parte de de One Lake y de y de Plepool, etc entonces si ahí hay un único tenal sería mucho más fácil tener el linaje pero eso es algo que tenemos que ver o sea tecnológicamente todavía no lo tenemos claro nos hablaron de que en arquitectura existen ciertos spoes de data, no no sé si nos puedan eh compartir un poco Eh eso cómo cómo se han definido cómo se han juntado ciertos dominios para que haya un un spoke Sales, parte del BNC de comercial finanzas de marketing hemos intentado a buscar un Spock para cada una de estas series, pero al final entendíamos que eran los que iban a a trabajar más en conjunto y no seía más fácil no también en el futuro y explortar los datos en un conjunto de table entonces hemos buscado tener una persona de contacto para cuando algún tipo de estos hay alguna incidencia o ellos necesitan soporte por nuestra parte es tener solo una persona de contacto no porque a veces si tenemos a todo el mundo que todo el mundo nos puede contactar se convierte un poco un caos tanto para nosotros como para ellos no porque a lo mejor esperamos que el ejemplo no dos personas de la misma y nos pueden preguntar lo mismo en general que responde a las dos personas y si al final tenemos una persona que centraliza dentro del área como una comunicación mucho más sencilla y no tenemos que estar ni duplicando mensajes ni ni el ruido que nos hace nosotros en el chat pues está tan elevado. Esa bueno nos comentaron que quizá se ha suspendido un poco el el el sí, la la frecuencia de trabajo, porque creo que al parecer ha sido absorbida por Alberto Aladid, pero luego con el cambio de de algún directivo bueno como que se quedó en pausa ahí no sé si nos puedan comentar un poco sobre qué tipo de trabajo se hace o o o si se. Aquí sería más, el que está asistiendo a este tipo de a mí lo hablo con David lo que tenía que es era mucho más enfocado en el transaccional, porque esto que nosotros estamos pintando aquí y que hemos definido desde cero y tal en el transaccional si algún momento existió, pues quedó muy desactualizado es decir no tenemos un mapa como tal de de sistemas o de arquitectura pero bueno bueno no quiero abrir más más frentes no pero en el mundo transaccional pues con toda la renovación Cloud y tal se quiere trabajar fuerte no en en en todo este mapa de sistemas y tal en la parte de o sea lo la forma en la que participamos nosotros en estos foros fue explicando, pues cómo habíamos definido esta esta arquitectura qué diferencias tiene respecto de la arquitectura transaccional por ejemplo en hay diferencias en en cómo definimos una arquitectura analítica, porque en un transaccional por ejemplo la los datos de desarrollo son datos siempre con bueno con con datos Fake no de de clientes, pero en un entorno analítico en desarrollo, tú tienes que tener datos que sean representativos de de tu población y y en preproducción todavía más no porque si quieres desarrollar algún tipo de solución analítica, no te sirve tener tres datos y que uno sea yo qué sé un un Fake no Paquito no sé qué el otro y y con mala calidad no como como ocurre hoy en el Ramsaccionar entonces lo que hemos hecho básicamente es compartir como enfocamos nosotros a la plataforma, el arquitectura de la plataforma analítica y los los desafíos que nos hemos encontrado pues bueno yo por ahora no tengo prepararé entonces. Cada una de las categorías y y le les pasaré, digamos cuáles han sido seleccionadas para ver un poco Eh, sí, por qué se ha llegado a a calificar no en en los grados de avance desde el cero al creo que al tres eh. Así que bueno, eso sería todo todo todo por ahora de mi parte, no sé si Roy tengan alguna pregunta. yo creo que serían los dos puntos luego si hay alguna cosa más pues ya os iremos diciendo no decir que son socorra y que que nos va ayudar un poco para autoría Bueno aquí podemos podemos avanzaros la documentación que hay en Confluencer casacentric.lu ya podemos echar un vistazo y ver un poco cómo está estructurado no y así ya por si tenemos ya cualquier duda si ya muy tal pues venimos venimos preparados con ella acceso supongo que ese ese mismo acceso de confluent donde está por ejemplo el el Data Quality Framework allí también está