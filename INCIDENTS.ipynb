{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069e7239",
   "metadata": {},
   "source": [
    "## ðŸ”¹ ConfiguraciÃ³n inicial (resumen)\n",
    "\n",
    "- `%run templates/config.py` y `%run templates/models.py`: carga configuraciones y modelos externos.  \n",
    "- `warnings.filterwarnings(...)`: oculta avisos de **openpyxl** y medias en arrays vacÃ­os.  \n",
    "- `comparison_date = '2025-09-01'`: fecha base para comparaciones temporales.  \n",
    "- `ticket_cols`: mapea columnas clave entre **incidents**, **problems** y **problem_tasks**.  \n",
    "- `time_cols_pairs`: define pares de fechas para medir duraciones (CREATEDâ†’RESOLVED, OUTAGE_BEGINâ†’CLOSED, etc.).  \n",
    "- `patterns`: regex para identificar IDs de tickets (PRB, INC, PTASK, CHG).  \n",
    "- `trace_changes`: diccionario para identificar los cambios en las tablas.  \n",
    "- Comentarios: *Aztech EspaÃ±a* (`aztecil/aztecsl`) y auditorÃ­a IberoLatam (ES, PT, CO, BR, AR, MX).  \n",
    "- `logger.info('START')`: marca inicio de ejecuciÃ³n y transformaciones en logs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44534da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run templates/config.py\n",
    "%run templates/models.py\n",
    "\n",
    "import warnings\n",
    "# Suppress specific warning from openpyxl\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl.styles.stylesheet\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\")\n",
    "\n",
    "comparison_date = '2025-09-01'\n",
    "\n",
    "ticket_cols = {\n",
    "    'df_incidents':('NUMBER','PROBLEM'),\n",
    "    'df_problem':('NUMBER','CREATED_OUT_OF_INCIDENT'),\n",
    "    'df_problem_tasks':('NUMBER','PROBLEM'),\n",
    "}\n",
    "\n",
    "time_cols_pairs = {\n",
    "    'df_incidents':[('CREATED','RESOLVED'),('CREATED','CLOSED'),('RESOLVED','CLOSED'),('OUTAGE_BEGIN','OUTAGE_END'),('OUTAGE_BEGIN','RESOLVED'),('OUTAGE_BEGIN','CLOSED')],\n",
    "    'df_problem':[('CREATED','RESOLVED'),('CREATED','CLOSED'),('RESOLVED','CLOSED'),('OUTAGE_BEGIN','OUTAGE_END'),('OUTAGE_BEGIN','RESOLVED'),('OUTAGE_BEGIN','CLOSED')],\n",
    "    'df_problem_tasks':[('CREATED','RESOLVED'),('CREATED','CLOSED'),('RESOLVED','CLOSED')],\n",
    "}\n",
    "\n",
    "patterns = [\n",
    "    r'PRB\\d{7}',   # Matches 'PRB' followed by 7 digits\n",
    "    r'INC\\d{8}',   # Matches 'INC' followed by 8 digits\n",
    "    r'PTASK\\d{7}', # Matches 'PTASK' followed by 7 digits\n",
    "    r'CHG\\d{7}'    # Matches 'CHG' followed by 7 digits\n",
    "]\n",
    "\n",
    "trace_changes = {}\n",
    "casos_audit_descrip = {} # Dictionary to hold descriptions of audit cases\n",
    "\n",
    "logger.info('START')\n",
    "logger_transform.info('START')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911216c",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Carga y depuraciÃ³n de datos\n",
    "\n",
    "- `log_separator(...)`: marca en los logs el inicio de secciones (**MAIN LOAD DATA**, **MAIN DROP DUPLICATES**).  \n",
    "- `load_from_pickle = False`: define si se cargan datos desde pickle o se procesan de nuevo.  \n",
    "- `load_and_process_data(...)`: carga los dataframes principales:\n",
    "  -  `df_incidents`, `df_problem`, `df_problem_tasks`.  \n",
    "  -  `df_created_incidents`, `df_resolved_incidents`, `df_problems_pbi`.\n",
    "- `original_columns`: diccionario con las columnas iniciales de cada dataframe (para referencia y trazabilidad).  \n",
    "- `original_shapes`: diccionario con las dimensiones iniciales de cada dataframe (filas, columnas).  \n",
    "- `handle_duplicates(df_incidents, 'df_incidents')`: elimina duplicados en la tabla **df_incidents**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dccd4878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(514006, 32) lectura\n",
      "(512027, 32) filtros previos\n",
      "(512027, 58) inicio\n",
      "(147833, 58) despues _Grupo_RES_all\n",
      "(14716, 58) despues _Grupo_No1LVL_all\n",
      "(14498, 58) despues _WaD_Closed\n",
      "(14498, 58) despues _Cancelled_Closed\n",
      "(1155, 58) despues _Glob_P4\n",
      "(1153, 58) despues _IS_Parent\n",
      "(1153, 58) despues _IS_FCR\n",
      "(902, 58) despues _IS_COMPANY\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FUNCIONES AUXILIARES\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def clean_str_compact(x: str) -> str:\n",
    "    \"\"\"Convierte texto a mayÃºsculas y elimina espacios/tabs/saltos de lÃ­nea.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \"\", x.upper().strip()) if isinstance(x, str) else x\n",
    "\n",
    "\n",
    "def apply_filters(df: pd.DataFrame, filters: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica filtros secuenciales basados en columnas booleanas.\n",
    "    Cada elemento de `filters` debe ser el nombre de una columna booleana.\n",
    "    \"\"\"\n",
    "    print(df.shape, \"inicio\")\n",
    "    for col in filters:\n",
    "        mask_numbers = df.loc[df[col], \"NUMBER\"].unique()\n",
    "        df = df[df[\"NUMBER\"].isin(mask_numbers)]\n",
    "        print(df.shape, f\"despues {col}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. CARGAR DICCIONARIO DE MAPEOS (Equipo -> Grupo)\n",
    "# ============================================================\n",
    "\n",
    "df_grupos_1lvl = pd.read_excel(\n",
    "    os.path.join(raw_data, \"grupos_audit.xlsx\"), sheet_name=\"1LVL\", dtype=str\n",
    ")\n",
    "df_grupos_IT = pd.read_excel(\n",
    "    os.path.join(raw_data, \"grupos_audit.xlsx\"), sheet_name=\"IT_TEAMS\", dtype=str\n",
    ")\n",
    "\n",
    "df_grupos = pd.concat([df_grupos_1lvl, df_grupos_IT], ignore_index=True)\n",
    "df_grupos[[\"Equipo\", \"Grupo\"]] = df_grupos[[\"Equipo\", \"Grupo\"]].map(clean_str_compact)\n",
    "\n",
    "map_dict = df_grupos.set_index(\"Equipo\")[\"Grupo\"].to_dict()\n",
    "\n",
    "# ============================================================\n",
    "# 2. CARGAR Y PREPARAR INCIDENCIAS\n",
    "# ============================================================\n",
    "\n",
    "load = True\n",
    "try:\n",
    "    log_separator(\"Data_INC.xlsx Source -> ServiceNow\", level=2)\n",
    "    logger.info(\"Loading and processing Data_INC.xlsx\")\n",
    "\n",
    "    # Ruta al pickle y fallback a Excel si no existe\n",
    "    pickle_path = os.path.join(raw_data, \"init_Data_INC.pkl\")\n",
    "    if os.path.exists(pickle_path) and load:\n",
    "        logger.info(f\"Loading Data_INC from pickle: {pickle_path}\")\n",
    "        df = pd.read_pickle(pickle_path)\n",
    "    else:\n",
    "        excel_path = os.path.join(raw_data, \"Data_INC.xlsx\")\n",
    "        logger.info(f\"Loading Data_INC from Excel: {excel_path}\")\n",
    "        df = pd.read_excel(excel_path)\n",
    "        df.to_pickle(pickle_path)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading Data_INC: {e}\")\n",
    "\n",
    "df = pd.read_pickle(os.path.join(raw_data, \"init_Data_INC.pkl\"))\n",
    "df_origen = df.copy()\n",
    "\n",
    "\n",
    "def tw_df(df):\n",
    "    return (\n",
    "        df.pipe(clean_headers)\n",
    "        .pipe(\n",
    "            set_df_column_order,\n",
    "            [\n",
    "                \"NUMBER\",\n",
    "                \"PROBLEM\",\n",
    "                \"STATE\",\n",
    "                \"LAST_ASSIGNMENT_GROUP\",\n",
    "                \"ASSIGNMENT_GROUP\",\n",
    "                \"CREATOR_GROUP\",\n",
    "            ],\n",
    "        )\n",
    "        .loc[\n",
    "            lambda x: (x['CREATED'].dt.year > 2022)\n",
    "            # & (x['CREATED'] < pd.to_datetime('2025-09-01'))\n",
    "        ]\n",
    "        .pipe(sort_values_of_incidents)\n",
    "    )\n",
    "\n",
    "print(df.shape, \"lectura\")\n",
    "df = tw_df(df)\n",
    "print(df.shape , \"filtros previos\")\n",
    "\n",
    "\n",
    "# Metadatos por grupo (agrupado por NUMBER)\n",
    "df[\"_ORDER\"] = df.groupby(\"NUMBER\").cumcount()\n",
    "df[\"_SIZE\"] = df.groupby(\"NUMBER\")[\"NUMBER\"].transform(\"size\")\n",
    "df[\"_LAST\"] = df[\"_ORDER\"] == (df[\"_SIZE\"] - 1)\n",
    "\n",
    "# ============================================================\n",
    "# 3. LIMPIAR Y MAPEAR COLUMNAS DE SERVICENOW\n",
    "# ============================================================\n",
    "\n",
    "group_cols = [\"LAST_ASSIGNMENT_GROUP\", \"ASSIGNMENT_GROUP\", \"CREATOR_GROUP\"]\n",
    "group_in_dict = [\"_Grupo_LAG\", \"_Grupo_ASG\", \"_Grupo_CRG\"]\n",
    "\n",
    "# Limpieza de strings\n",
    "df[group_cols] = df[group_cols].map(clean_str_compact)\n",
    "casos_audit[\"INC_Company_null\"] = df[df['COMPANY'].isnull()]\n",
    "casos_audit_descrip[\"INC_Company_null\"] = \"Casos sin empresa 'vacio'\"\n",
    "\n",
    "df['COMPANY'] = df['COMPANY'].fillna(\"_VACIO_\").apply(standardize_company_name)\n",
    "\n",
    "to_AZS = [\"ALLIANZSEGUROS SA\", \"ALLIANZ SEGUROS SA\", \"ALLIANZ COMPANIA DE SEGUROS Y REASEGUROS SA\"]\n",
    "df['COMPANY'] = df['COMPANY'].replace(to_AZS, 'ALLIANZ SEGUROS SA')\n",
    "keep = [\"ALLIANZ SEGUROS SA\",\"ALLIANZ SPAIN EXTERNAL\",\"ALLIANZ TECHNOLOGY SL\",\"BBVA\"]\n",
    "\n",
    "# Mapear con fallback (\"\" si no existe en el dict)\n",
    "df[group_in_dict] = df[group_cols].map(lambda x: map_dict.get(x, \"\"))\n",
    "\n",
    "# Flags de grupo\n",
    "df[\"_Grupo_RES_any\"] = (df[group_in_dict] != \"\").any(axis=1)\n",
    "df[\"_Grupo_RES_all\"] = (df[group_in_dict] != \"\").all(axis=1)\n",
    "\n",
    "df[\"_Grupo_No1LVL_any\"] = (df[group_in_dict] != \"1LVL-OTROS\").any(axis=1)\n",
    "df[\"_Grupo_No1LVL_all\"] = (df[group_in_dict] != \"1LVL-OTROS\").all(axis=1)\n",
    "\n",
    "df[\"_Grupo_GLOB_any\"] = (df[group_in_dict] == \"GLOB-OTROS\").any(axis=1)\n",
    "df[\"_Grupo_GLOB_all\"] = (df[group_in_dict] == \"GLOB-OTROS\").all(axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# 4. REGLAS DE ÃšLTIMO VALOR\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    # A) Ãšltimos valores de grupos\n",
    "    last_values = df.groupby(\"NUMBER\")[group_cols].last()\n",
    "    last_mapped = last_values.map(lambda x: x in map_dict).add_prefix(\"_IS_Last_\")\n",
    "    df = df.merge(last_mapped, on=\"NUMBER\", how=\"left\")\n",
    "\n",
    "    # B) Ãšltimo estado/causa\n",
    "    last_values = df.groupby(\"NUMBER\")[[\"STATE\", \"CAUSE_CODE\"]].last()\n",
    "    df[\"_LAST_STATE_CLOSED\"] = df[\"NUMBER\"].map(last_values[\"STATE\"] != \"CLOSED\")\n",
    "    df[\"_LAST_CAUSE_CODE_WaD\"] = df[\"NUMBER\"].map(\n",
    "        last_values[\"CAUSE_CODE\"] != \"Works as designed\"\n",
    "    )\n",
    "    df[\"_LAST_CAUSE_CODE_Cancelled\"] = df[\"NUMBER\"].map(\n",
    "        last_values[\"CAUSE_CODE\"] != \"Cancelled\"\n",
    "    )\n",
    "    df[\"_WaD_Closed\"] = df[\"_LAST_STATE_CLOSED\"] & df[\"_LAST_CAUSE_CODE_WaD\"]\n",
    "    df[\"_Cancelled_Closed\"] = (\n",
    "        df[\"_LAST_STATE_CLOSED\"] & df[\"_LAST_CAUSE_CODE_Cancelled\"]\n",
    "    )\n",
    "\n",
    "    # C) Ãšltimos GLOB/Prioridad\n",
    "    last_values = df.groupby(\"NUMBER\")[[\"_Grupo_GLOB_all\", \"CURRENT_PRIORITY\"]].last()\n",
    "    mask_excluir = last_values[\"_Grupo_GLOB_all\"] & last_values[\n",
    "        \"CURRENT_PRIORITY\"\n",
    "    ].isin([\"P4\", \"P3\"])\n",
    "    df[\"_Glob_P4\"] = df[\"NUMBER\"].map(~mask_excluir)\n",
    "\n",
    "    # D)\n",
    "    # last_values = df.groupby(\"NUMBER\")[[\"STATE\", \"CAUSE_CODE\"]].last()\n",
    "    # df[\"_LAST_STATE_CLOSED\"] = df[\"NUMBER\"].map(last_values[\"STATE\"] != \"CLOSED\")\n",
    "    # df[\"_LAST_CAUSE_CODE_WaD\"] = df[\"NUMBER\"].map(\n",
    "    #     last_values[\"CAUSE_CODE\"] != \"Works as designed\"Âº\n",
    "    # )\n",
    "    # df[\"_WaD_Closed\"] = df[\"_LAST_STATE_CLOSED\"] & df[\"_LAST_CAUSE_CODE_WaD\"]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Flags agregados de Ãºltimo\n",
    "prefix = \"_IS_Last_\"\n",
    "df[\"_Grupo_LAST_any\"] = df.filter(like=prefix).any(axis=1)\n",
    "df[\"_Grupo_LAST_all\"] = df.filter(like=prefix).all(axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# 5. FILTROS SECUENCIALES\n",
    "# ============================================================\n",
    "\n",
    "# Define masks based on conditions\n",
    "df['_IS_Parent'] = df['PARENT_INCIDENT'].fillna('') == '' # remove childs\n",
    "df['_IS_FCR'] = df['FCR'] != True\n",
    "df['_IS_COMPANY'] = df['COMPANY'].isin(keep)\n",
    "\n",
    "filters = [\n",
    "    \"_Grupo_RES_all\",\n",
    "    \"_Grupo_No1LVL_all\",\n",
    "    \"_WaD_Closed\",\n",
    "    \"_Cancelled_Closed\",\n",
    "    \"_Glob_P4\",\n",
    "    \"_IS_Parent\",\n",
    "    \"_IS_FCR\",\n",
    "    \"_IS_COMPANY\"\n",
    "]\n",
    "df = apply_filters(df, filters)\n",
    "\n",
    "# ============================================================\n",
    "# 6. ORDEN FINAL\n",
    "# ============================================================\n",
    "\n",
    "df_incidents = df.pipe(sort_values_of_incidents, \"_SIZE\")\n",
    "\n",
    "try:\n",
    "    del df, df_grupos, df_grupos_IT, df_grupos_1lvl\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "incident_tickets = df_incidents['NUMBER'].unique()\n",
    "problem_tickets = df_incidents[\"PROBLEM\"].unique()\n",
    "\n",
    "# cancelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e7b943-b373-4084-8fba-3c0970ae4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fastparquet\n",
    "\n",
    "import pyarrow\n",
    "import fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb5d2f-f203-4c95-a58f-6c606037b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Read the Excel file\n",
    "# name = 'Data_INC'\n",
    "name = 'Data_INC' # problem, problem_task, CRE_INC\n",
    "\n",
    "file_path = os.path.join(raw_data, f\"{name}.xlsx\")\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7177a98d-27e4-404c-8b17-8b7d493aff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(columns=\"Affected User\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5a53b7c-fb20-4e39-bb6d-75660988b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Determine split size\n",
    "# Assuming each row is approximately 1 KB, calculate rows per file for 100 MB limit\n",
    "approx_row_size_kb = 1  # Adjust based on your data\n",
    "rows_per_file = (24 * 1024) // approx_row_size_kb\n",
    "\n",
    "# Step 3: Split the DataFrame\n",
    "num_files = (len(df) // rows_per_file) + 1\n",
    "print(num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c1b35e3-1186-4a4f-88c1-a2654be63445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully split into 21 files.\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to store smaller files\n",
    "output_dir = f'{name}_split_files'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 4: Save smaller files in Parquet format\n",
    "for i in range(num_files):\n",
    "    start_row = i * rows_per_file\n",
    "    end_row = min((i + 1) * rows_per_file, len(df))\n",
    "    df_split = df.iloc[start_row:end_row]\n",
    "    output_file = os.path.join(output_dir, f'{name}_split_file_{i+1}.parquet')\n",
    "    try:\n",
    "        df_split.to_parquet(output_file, engine='fastparquet', compression='gzip')\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file {output_file}: {e}\")\n",
    "\n",
    "print(f'Successfully split into {num_files} files.')\n",
    "\n",
    "# # Step 5: Merge files (when needed)\n",
    "# # Example of merging back the files\n",
    "# merged_df = pd.DataFrame()\n",
    "# for i in range(num_files):\n",
    "#     input_file = os.path.join(output_dir, f'{}split_file_{i+1}.parquet')\n",
    "#     df_part = pd.read_parquet(input_file)\n",
    "#     merged_df = pd.concat([merged_df, df_part], ignore_index=True)\n",
    "\n",
    "# print('Successfully merged files back into a single DataFrame.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = os.path.join(raw_data, \"Data_INC.xlsx\")\n",
    "df_origen = pd.read_excel(excel_path)\n",
    "df_origen[df_origen[\"Number\"]==\"INC29720425\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0c025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71fe69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc02519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marks(df):\n",
    "    df[\"_ORDER\"] = df.groupby(\"NUMBER\").cumcount()\n",
    "    df[\"_SIZE\"] = df.groupby(\"NUMBER\")[\"NUMBER\"].transform(\"size\")\n",
    "    df[\"_LAST\"] = df[\"_ORDER\"] == (df[\"_SIZE\"] - 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_incidents = (\n",
    "    df_incidents[[col for col in df_incidents.columns if not col.startswith(\"_\")]]\n",
    "    .pipe(process_not_closed_business_optimized, time_cols_pairs['df_incidents'])\n",
    "    .pipe(exceed_bands_priority_CRE_RES, 'CURRENT_PRIORITY')\n",
    "    .pipe(marks)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_incidents[[col for col in df_incidents.columns if not col.startswith(\"_\")]].to_excel(\"validar_base.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2304e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descartados = df_origen.pipe(clean_headers)[~df_origen['NUMBER'].isin(df_incidents['NUMBER'])]\n",
    "df_descartados = df_descartados.pipe(sort_values_of_incidents)\n",
    "df_descartados.to_excel(\"validar_descartados.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_incidents.COMPANY.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1866f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f98f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabulate_df(analyze_dataframe_columns_export(df), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebed601",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_separator(\n",
    "    title=\"Section: [MAIN LOAD DATA] Description: [Loading and processing main data]\"\n",
    ")\n",
    "\n",
    "load_from_pickle = True\n",
    "\n",
    "(\n",
    "    df_created_incidents,\n",
    "    df_resolved_incidents,\n",
    "    df_problems_pbi,\n",
    "    df_problem_tasks,\n",
    "    df_problem,\n",
    "    # df_incidents,\n",
    ") = load_and_process_data(load_from_pickle=load_from_pickle)\n",
    "\n",
    "original_columns = {\n",
    "    # SECUNDARY (PowerBI)\n",
    "    \"df_created_incidents\": df_created_incidents.columns.tolist(),\n",
    "    \"df_resolved_incidents\": df_resolved_incidents.columns.tolist(),\n",
    "    \"df_problems_pbi\": df_problems_pbi.columns.tolist(),\n",
    "    # PRINCIPAL (ServiceNow)\n",
    "    \"df_incidents\": df_incidents.columns.tolist(),\n",
    "    \"df_problem\": df_problem.columns.tolist(),\n",
    "    \"df_problem_tasks\": df_problem_tasks.columns.tolist(),\n",
    "}\n",
    "original_shapes = {\n",
    "    # SECUNDARY (PowerBI)\n",
    "    \"df_created_incidents\": df_created_incidents.shape,\n",
    "    \"df_resolved_incidents\": df_resolved_incidents.shape,\n",
    "    \"df_problems_pbi\": df_problems_pbi.shape,\n",
    "    # PRINCIPAL (ServiceNow)\n",
    "    \"df_incidents\": df_incidents.shape,\n",
    "    \"df_problem\": df_problem.shape,\n",
    "    \"df_problem_tasks\": df_problem_tasks.shape,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b917112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_changes['1_Carga'] = print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43029b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_incidents.to_excel(\"test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_separator(title=\"Section: [MAIN DROP DUPLICATES] Description: [Drop duplicate rows]\")\n",
    "log_separator(\"TABLE: 'df_incidents'\", 3)\n",
    "df_incidents = handle_duplicates(df_incidents, 'df_incidents').pipe(\n",
    "    sort_values_of_incidents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_changes['2_INC_Duplicados'] = print_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5407e4d",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Relaciones entre dataframes y validaciÃ³n\n",
    "\n",
    "- `dataframes_relationships`: metadatos con origen (**ServiceNow/PowerBI**), clave primaria (`pk`) y clave forÃ¡nea (`fk`) de cada tabla.  \n",
    "- `dataframes`: diccionario con los dataframes cargados (incidents, problems, tasks, created/resolved incidents, problems PBI).  \n",
    "  - Nota: `df_incidents_grouped` se generarÃ¡ mÃ¡s adelante (Ãºltimo UPDATE por ticket).  \n",
    "- `create_structured_relationship_map(...)`: construye un mapa estructurado de relaciones entre tablas.  \n",
    "- `get_flat_relationships(...)`: aplana el mapa en una lista de relaciones simples.  \n",
    "- `print_valid_relationships(...)`: muestra las relaciones vÃ¡lidas identificadas.  \n",
    "- `create_table_comparisons(...)`: genera comparaciones de datos entre tablas relacionadas.  \n",
    "- `try/except del dataframes`: libera memoria eliminando el diccionario si existe.  \n",
    "- Salida final:  \n",
    "  - Total de relaciones vÃ¡lidas encontradas.  \n",
    "  - NÃºmero de relaciones con coincidencias de datos.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata\n",
    "dataframes_relationships = {\n",
    "    'df_incidents': {'db': 'ServiceNow', 'pk': 'NUMBER', 'fk': 'PROBLEM'},\n",
    "    'df_problem': {'db': 'ServiceNow', 'pk': 'NUMBER', 'fk': 'CREATED_OUT_OF_INCIDENT'},\n",
    "    'df_problem_tasks': {'db': 'ServiceNow', 'pk': 'NUMBER', 'fk': 'PROBLEM'},\n",
    "    'df_created_incidents': {'db': 'PowerBI', 'pk': 'NUMBER', 'fk': 'PROBLEM'},\n",
    "    'df_resolved_incidents': {'db': 'PowerBI', 'pk': 'NUMBER', 'fk': 'PROBLEM'},\n",
    "    'df_problems_pbi': {'db': 'PowerBI','pk': 'NUMBER','fk': 'CREATED_OUT_OF_INCIDENT'},\n",
    "    'df_incidents_grouped': {'db': 'ServiceNow', 'pk': 'NUMBER', 'fk': 'PROBLEM'},\n",
    "}\n",
    "\n",
    "dataframes = {\n",
    "    'df_incidents': df_incidents,\n",
    "    'df_problem': df_problem,\n",
    "    'df_problem_tasks': df_problem_tasks,\n",
    "    'df_created_incidents': df_created_incidents,\n",
    "    'df_resolved_incidents': df_resolved_incidents,\n",
    "    'df_problems_pbi': df_problems_pbi,\n",
    "}\n",
    "\n",
    "# relationship map\n",
    "relationship_map = create_structured_relationship_map(dataframes_relationships)\n",
    "\n",
    "# relationships\n",
    "flat_relationships = get_flat_relationships(relationship_map)\n",
    "print(f\"\\nTotal semantically valid relationships identified: {len(flat_relationships)}\")\n",
    "print_valid_relationships(flat_relationships)\n",
    "\n",
    "# comparison table\n",
    "table_comparisons = create_table_comparisons(flat_relationships, dataframes)\n",
    "casos_audit['PBI_vs_ServiceNow'] = table_comparisons\n",
    "casos_audit_descrip['Relaciones con datos que coinciden'] = \"Relaciones entre tablas donde las claves forÃ¡neas y primarias tienen coincidencias en los datos.\"\n",
    "# print(f\"Relationships with data matches: {len(table_comparisons)}\")\n",
    "\n",
    "try:\n",
    "    del table_comparisons\n",
    "    del dataframes\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79324641",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Enriquecimiento de columnas de fecha\n",
    "\n",
    "- **FunciÃ³n `create_dt_time_wvariants(df)`**  \n",
    "  - Crea variantes adicionales para cada columna `datetime` en el dataframe (excepto `UPDATED`, `OUTAGE_BEGIN`, `OUTAGE_END`).  \n",
    "  - Genera nuevas columnas:\n",
    "    - `_{col}_year`: aÃ±o.  \n",
    "    - `_{col}_month`: mes numÃ©rico.  \n",
    "    - `_{col}_period`: periodo en formato mensual (`YYYY-MM`).  \n",
    "    - `_{col}_day_name`: nombre del dÃ­a de la semana.  \n",
    "    - `_{col}_isnull`: indicador de valores nulos.  \n",
    "    - `_{col}_str`: fecha en formato string (`YYYY-MM-DD HH:MM`).  \n",
    "\n",
    "  - Se enriquece con variantes de tiempo cada dataset principal:  \n",
    "    - `df_incidents`, `df_problem`, `df_problem_tasks`  \n",
    "    - `df_created_incidents`, `df_resolved_incidents`, `df_problems_pbi`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c735d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents = df_incidents.pipe(create_dt_time_wvariants)\n",
    "df_problem = df_problem.pipe(create_dt_time_wvariants)\n",
    "df_problem_tasks = df_problem_tasks.pipe(create_dt_time_wvariants)\n",
    "\n",
    "df_created_incidents = df_created_incidents.pipe(create_dt_time_wvariants)\n",
    "df_resolved_incidents = df_resolved_incidents.pipe(create_dt_time_wvariants)\n",
    "df_problems_pbi = df_problems_pbi.pipe(create_dt_time_wvariants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc93199",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_changes['3_ColumnasFechas'] = print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd58466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean columns\n",
    "df_incidents['COMPANY'] = df_incidents['COMPANY'].apply(standardize_company_name)\n",
    "df_incidents['_SERVICE_OFFERING'] = df_incidents['SERVICE_OFFERING'].pipe(clean_service_offering_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015d3ac",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Filtro de grupos no vÃ¡lidos (A.TEC.ES)\n",
    "\n",
    "- **FunciÃ³n `filtrar_grupos(df, columnas, fuente, casos_audit, casos_audit_descrip, clave_base)`**  \n",
    "  - Revisa si los valores de las columnas especificadas **no comienzan por** `\"A.TEC.ES.\"`.  \n",
    "  - Filtra esas filas y aÃ±ade metadatos:\n",
    "    - `_FUENTE`: nombre de la tabla origen.  \n",
    "    - `_TEST`: columna evaluada + condiciÃ³n aplicada.  \n",
    "\n",
    "- **Aplicaciones en datasets PBI**  \n",
    "  - `df_created_incidents`: verifica `\"CREATOR_GROUP\"` y `\"ASSIGNMENT_GROUP\"`.  \n",
    "  - `df_resolved_incidents`: verifica `\"CREATOR_GROUP\"` y `\"ASSIGNMENT_GROUP\"`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filtrar_grupos(df, columnas, fuente, casos_audit, casos_audit_descrip, clave_base):\n",
    "#     \"\"\"\n",
    "#     Filtra filas de df donde las columnas no empiezan por 'A.TEC.ES.'.\n",
    "#     AÃ±ade metadatos y actualiza los diccionarios de auditorÃ­a.\n",
    "    \n",
    "#     Args:\n",
    "#         df (pd.DataFrame): dataframe de entrada\n",
    "#         columnas (list): columnas a verificar ['CREATOR_GROUP','ASSIGNMENT_GROUP',...]\n",
    "#         fuente (str): nombre de la fuente para asignar en la columna '_FUENTE'\n",
    "#         casos_audit (dict): diccionario donde se guarda el resultado\n",
    "#         casos_audit_descrip (dict): diccionario con descripciones\n",
    "#         clave_base (str): clave base para usar en el dict (ej: 'PBI-INC_CreaNo_A-TEC-ES')\n",
    "#     \"\"\"\n",
    "#     resultados = []\n",
    "#     for col in columnas:\n",
    "#         mask = ~df[col].str.startswith(\"A.TEC.ES.\", na=False)\n",
    "#         resultados.append(\n",
    "#             df[mask].assign(_FUENTE=fuente, _TEST=f\"{col} not start with A.TEC.ES.\")\n",
    "#         )\n",
    "\n",
    "#     result = pd.concat(resultados, axis=0)\n",
    "#     casos_audit[clave_base] = result\n",
    "#     casos_audit_descrip[clave_base] = f\"{clave_base}: Filtrado por {', '.join(columnas)}\"\n",
    "#     return casos_audit, casos_audit_descrip\n",
    "\n",
    "\n",
    "# # df_created_incidents\n",
    "# casos_audit, casos_audit_descrip = filtrar_grupos(\n",
    "#     df_created_incidents,\n",
    "#     [\"CREATOR_GROUP\", \"ASSIGNMENT_GROUP\"],\n",
    "#     fuente=\"df_created_incidentsPBI\",\n",
    "#     casos_audit=casos_audit,\n",
    "#     casos_audit_descrip=casos_audit_descrip,\n",
    "#     clave_base=\"PBI-INC_CreaNo_A-TEC-ES\",\n",
    "# )\n",
    "\n",
    "# # df_resolved_incidents\n",
    "# casos_audit, casos_audit_descrip = filtrar_grupos(\n",
    "#     df_resolved_incidents,\n",
    "#     [\"CREATOR_GROUP\", \"ASSIGNMENT_GROUP\"],\n",
    "#     fuente=\"df_resolved_incidentsPBI\",\n",
    "#     casos_audit=casos_audit,\n",
    "#     casos_audit_descrip=casos_audit_descrip,\n",
    "#     clave_base=\"PBI-INC_ResNo_A-TEC-ES\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce827f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_cause_code(df_created, df_resolved):\n",
    "    \"\"\"\n",
    "    Compara los CAUSE_CODE entre incidentes creados y resueltos.\n",
    "\n",
    "    Args:\n",
    "        df_created (pd.DataFrame): DataFrame de incidentes creados\n",
    "        df_resolved (pd.DataFrame): DataFrame de incidentes resueltos\n",
    "        casos_audit (dict): Diccionario donde se guarda el resultado\n",
    "\n",
    "    Returns:\n",
    "        dict: casos_audit actualizado\n",
    "    \"\"\"\n",
    "\n",
    "    def preparar(df, fuente):\n",
    "        return (\n",
    "            df.CAUSE_CODE.value_counts()\n",
    "            .reset_index()\n",
    "            .assign(\n",
    "                _PCT=lambda x: x[\"count\"] / x[\"count\"].sum(),\n",
    "                _FUENTE=fuente,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    df_created_proc = preparar(df_created, \"df_created_incidentsPBI\")\n",
    "    df_resolved_proc = preparar(df_resolved, \"df_resolved_incidentsPBI\")\n",
    "\n",
    "    merged_df = df_created_proc.merge(\n",
    "        df_resolved_proc,\n",
    "        on=\"CAUSE_CODE\",\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_created\", \"_resolved\"),\n",
    "    )\n",
    "\n",
    "    result = (\n",
    "        merged_df.sort_values(by=\"count_resolved\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    result[\"created_minus_resolved\"] = (\n",
    "        result[\"count_created\"].fillna(0) - result[\"count_resolved\"].fillna(0)\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# df_created_incidents vs df_resolved_incidents\n",
    "key=\"PBI-INC_CreaVsRes\"\n",
    "casos_audit[key] = comparar_cause_code(df_created_incidents, df_resolved_incidents)\n",
    "casos_audit_descrip[key] = \"ComparaciÃ³n de CAUSE_CODE entre incidentes creados y resueltos (PowerBI)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filtrar_cause_code(df_created, df_resolved):\n",
    "#     \"\"\"\n",
    "#     Filtra registros donde CAUSE_CODE NO empieza por 'Works as designed'\n",
    "#     en incidentes creados y resueltos.\n",
    "\n",
    "#     Args:\n",
    "#         df_created (pd.DataFrame): DataFrame de incidentes creados\n",
    "#         df_resolved (pd.DataFrame): DataFrame de incidentes resueltos\n",
    "#         casos_audit (dict): diccionario donde guardar el resultado\n",
    "#         key (str): clave para guardar en casos_audit\n",
    "\n",
    "#     Returns:\n",
    "#         dict: casos_audit actualizado\n",
    "#     \"\"\"\n",
    "\n",
    "#     def preparar(df, fuente, rename_map=None):\n",
    "#         mask = ~df[\"CAUSE_CODE\"].str.startswith(\"Works as designed\", na=False)\n",
    "#         df_out = df[mask].assign(\n",
    "#             _FUENTE=fuente,\n",
    "#             _TEST=\"CAUSE_CODE dont start with Works as designed\"\n",
    "#         )\n",
    "#         if rename_map:\n",
    "#             df_out = df_out.rename(columns=rename_map)\n",
    "#         return df_out\n",
    "\n",
    "#     result_created = preparar(\n",
    "#         df_created, \n",
    "#         \"df_created_incidentsPBI\"\n",
    "#     )\n",
    "#     result_resolved = preparar(\n",
    "#         df_resolved,\n",
    "#         \"df_resolved_incidentsPBI\"\n",
    "#     )\n",
    "\n",
    "#     result = pd.concat([result_created, result_resolved], axis=0)\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# # df_created_incidents vs df_resolved_incidents\n",
    "# key=\"PBI-INC_CauseNo_WorkAsDesigned\"\n",
    "# casos_audit[key] = filtrar_cause_code(df_created_incidents, df_resolved_incidents)\n",
    "# casos_audit_descrip[key] = \"ComparaciÃ³n de CAUSE_CODE entre incidentes creados y resueltos (PowerBI)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ee0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(casos_audit.keys())\n",
    "# print(casos_audit_descrip.keys())\n",
    "# key=\"PBI-INC_CauseNo_WorkAsDesigned\"\n",
    "\n",
    "# display(casos_audit[key].head(),casos_audit[key].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5b7c0",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Filtro de incidentes de grupos espaÃ±oles\n",
    "\n",
    "- **FunciÃ³n `filtrar_incidentes_es(df_created, df_resolved, df_incidents, grupos_cols)`**  \n",
    "  - Identifica incidentes relacionados con **grupos espaÃ±oles**, usando informaciÃ³n de PowerBI.  \n",
    "  - Pasos principales:\n",
    "    1. **Recolecta grupos Ãºnicos** de interÃ©s desde `df_created_incidents` y `df_resolved_incidents`.  \n",
    "    2. **Filtra `df_incidents`** si alguno coincide en:\n",
    "       - `ASSIGNMENT_GROUP`  \n",
    "       - `LAST_ASSIGNMENT_GROUP`  \n",
    "       - `CREATOR_GROUP`  \n",
    "    3. Elimina incidentes sin `NUMBER`.  \n",
    "    4. Ordena resultados con `sort_values_of_incidents`.  \n",
    "\n",
    "- **Retorno**  \n",
    "  - `df_filtrado`: incidentes vÃ¡lidos pertenecientes a grupos espaÃ±oles.  \n",
    "  - `es_groups`: lista de grupos Ãºnicos identificados.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filtrar_incidentes_es(df_created, df_resolved, df_incidents, grupos_cols=None):\n",
    "#     \"\"\"\n",
    "#     Filtra incidentes relacionados con grupos espaÃ±oles (segÃºn PowerBI).\n",
    "\n",
    "#     Args:\n",
    "#         df_created (pd.DataFrame): incidentes creados\n",
    "#         df_resolved (pd.DataFrame): incidentes resueltos\n",
    "#         df_incidents (pd.DataFrame): dataset completo de incidentes\n",
    "#         sort_func (callable): funciÃ³n para ordenar los incidentes\n",
    "#         grupos_cols (list): columnas donde buscar grupos (default=['ASSIGNMENT_GROUP','CREATOR_GROUP'])\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: df_incidents filtrado y ordenado\n",
    "#     \"\"\"\n",
    "\n",
    "#     if grupos_cols is None:\n",
    "#         grupos_cols = [\"ASSIGNMENT_GROUP\", \"CREATOR_GROUP\"]\n",
    "\n",
    "#     # 1. Obtener todos los grupos Ãºnicos de interÃ©s\n",
    "#     es_groups = pd.concat([\n",
    "#         df_created[grupos_cols],\n",
    "#         df_resolved[grupos_cols]\n",
    "#     ], axis=0).stack().dropna().unique().tolist()\n",
    "\n",
    "#     # 2. Filtrar incidentes cuyos grupos coinciden\n",
    "#     mask = (\n",
    "#         df_incidents[\"ASSIGNMENT_GROUP\"].isin(es_groups)\n",
    "#         | df_incidents[\"LAST_ASSIGNMENT_GROUP\"].isin(es_groups)\n",
    "#         | df_incidents[\"CREATOR_GROUP\"].isin(es_groups)\n",
    "#     )\n",
    "#     df_lost = df_incidents.loc[~mask]\n",
    "#     df_filtrado = df_incidents.loc[mask]\n",
    "#     df_filtrado = df_filtrado[df_filtrado[\"NUMBER\"].notna()]  # asegurar claves vÃ¡lidas\n",
    "#     df_filtrado = df_filtrado.pipe(sort_values_of_incidents)\n",
    "\n",
    "#     return df_filtrado, df_lost, es_groups\n",
    "\n",
    "\n",
    "# df_incidents, df_lost, es_groups = filtrar_incidentes_es(df_created_incidents, df_resolved_incidents, df_incidents)\n",
    "# try:\n",
    "#     del df_lost\n",
    "# except NameError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace_changes['4_INC_Filtro_GruposRefEnPBI'] = print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4393573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_process_incidents(intermediate_data, df_incidents, load_from_pickle=False):\n",
    "    \"\"\"\n",
    "    Carga incidentes desde pickle o los procesa y guarda.\n",
    "\n",
    "    Args:\n",
    "        intermediate_data (str): carpeta donde guardar/cargar pickles\n",
    "        df_incidents (pd.DataFrame): dataset base de incidentes (si no hay pickle)\n",
    "        load_from_pickle (bool): si True intenta cargar desde pickle\n",
    "    \"\"\"\n",
    "    path = os.path.join(intermediate_data, \"incidents.pickle\")\n",
    "    path_created = os.path.join(intermediate_data, \"created_incidents.pickle\")\n",
    "\n",
    "    if load_from_pickle and os.path.exists(path):\n",
    "        df_incidents = pd.read_pickle(path)\n",
    "\n",
    "    else:\n",
    "        # Procesamiento encadenado con .pipe para mayor claridad\n",
    "        df_incidents = (\n",
    "            df_incidents.pipe(get_inc_count_rows)\n",
    "            .pipe(get_COLvals_countUnique_per_inc, func_col=\"PROBLEM\")\n",
    "            .pipe(get_COLvals_list_per_inc, func_col=\"PROBLEM\")\n",
    "            .pipe(get_COLvals_countUnique_per_inc, func_col=\"LAST_ASSIGNMENT_GROUP\")\n",
    "            .pipe(get_COLvals_list_per_inc, func_col=\"LAST_ASSIGNMENT_GROUP\")\n",
    "            .pipe(get_COLvals_countUnique_per_inc, func_col=\"ASSIGNMENT_GROUP\")\n",
    "            .pipe(get_bool_comparation, \"LAST_ASSIGNMENT_GROUP\", \"ASSIGNMENT_GROUP\")\n",
    "            .pipe(get_bool_comparation, \"CURRENT_PRIORITY\", \"PRIORITY\")\n",
    "            .pipe(get_bool_comparation, \"HIGHEST_PRIORITY\", \"CURRENT_PRIORITY\")\n",
    "            .pipe(flag_state_change)\n",
    "            .pipe(clean_LAST_ASSIGNMENT_GROUP)\n",
    "            .pipe(clean_ASSIGNMENT_GROUP)\n",
    "        )\n",
    "\n",
    "        # Guardar\n",
    "        df_incidents.to_pickle(path)\n",
    "\n",
    "\n",
    "    return df_incidents\n",
    "\n",
    "\n",
    "df_incidents = load_or_process_incidents(intermediate_data, df_incidents, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_created_incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f06833",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_changes['5_INC_FlagStateChange'] = print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1212f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_separator(title=\"Section: [MAIN GET GROUPS] Description: [Get groups based on INCIDENT-PROBLEM]\")\n",
    "log_separator(\"TABLE: 'df_incidents'\", level=3)\n",
    "\n",
    "df_incidents = set_pct_completed(df_incidents)\n",
    "df_incidents = process_incidents(df_incidents)\n",
    "\n",
    "# Filtrar directamente donde _PROBLEM_TO_NULL == True\n",
    "key = \"INC_ProblemToNull\"\n",
    "casos_audit[key] = df_incidents.query(\"_PROBLEM_TO_NULL == True\")\n",
    "casos_audit_descrip[key] = \"Incident que en UPDATED pasa de tener PROBLEM a NULL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_changes['6_INC_FromProblemToNull'] = print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09437478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_ticket_relations(dfs, relations):\n",
    "    \"\"\"\n",
    "    Analiza relaciones entre tickets en varios DataFrames (parent-child).\n",
    "\n",
    "    Args:\n",
    "        dfs (dict): diccionario {nombre: DataFrame}\n",
    "        relations (dict): diccionario {nombre: lista de pares de columnas}\n",
    "        log_separator (callable): funciÃ³n para loggear\n",
    "        logger (logging.Logger): logger activo\n",
    "    \"\"\"\n",
    "    log_separator(\n",
    "        title=\"Section: [MAIN TEST TICKET RELATION] Description: [DataFrame ticket relation analysis (parent-child)]\"\n",
    "    )\n",
    "    logger.info(f\"The relations are {relations}\")\n",
    "\n",
    "    result = {}\n",
    "    for name, df in dfs.items():\n",
    "        log_separator(f\"TABLE: '{name}'\", 3)\n",
    "        pairs = relations.get(name, [])\n",
    "        result[name] = log_and_get_nuniques(df, name, pairs)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "dfs = {\n",
    "    \"df_incidents\": df_incidents,\n",
    "    \"df_problem\": df_problem,\n",
    "    \"df_problem_tasks\": df_problem_tasks,\n",
    "}\n",
    "\n",
    "relations = {\n",
    "    \"df_incidents\": [(\"NUMBER\", \"PROBLEM\"), (\"PROBLEM\", \"NUMBER\")],\n",
    "    \"df_problem\": [(\"NUMBER\", \"CREATED_OUT_OF_INCIDENT\"), (\"CREATED_OUT_OF_INCIDENT\", \"NUMBER\")],\n",
    "    \"df_problem_tasks\": [(\"NUMBER\", \"PROBLEM\"), (\"PROBLEM\", \"NUMBER\")],\n",
    "}\n",
    "\n",
    "processed_dfs = analizar_ticket_relations(dfs, relations)\n",
    "\n",
    "# ahora puedes hacer:\n",
    "df_incidents = processed_dfs[\"df_incidents\"]\n",
    "df_problem = processed_dfs[\"df_problem\"]\n",
    "df_problem_tasks = processed_dfs[\"df_problem_tasks\"]\n",
    "\n",
    "try:\n",
    "    del dfs, processed_dfs, relations\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error al eliminar variables: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_changes['7_ALL_GetRelations'] = print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685505c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result make a mask group by number and get the NUMBERS where any of _CHANGE_IN_ROW its rows has True\n",
    "df_incidents = flag_PARENT_change(df_incidents)\n",
    "mask = df_incidents[df_incidents['_CHANGE_IN_ROW'] == True].NUMBER.unique()\n",
    "result = df_incidents[df_incidents.NUMBER.isin(mask)]\n",
    "cols = [\n",
    "    'NUMBER',\n",
    "    'UPDATED',\n",
    "    'PARENT_INCIDENT',\n",
    "    'CHILD_INCIDENTS',\n",
    "    '_CHANGE_IN_ROW',\n",
    "    '_TYPE_CHANGE',\n",
    "    '_SET_TICKET_PARENTS',\n",
    "    '_TOTAL_CHANGES',\n",
    "]\n",
    "\n",
    "casos_audit['INC_ParentChanges'] = result.pipe(set_df_column_order, cols).pipe(\n",
    "    sort_values_of_incidents, '_TOTAL_CHANGES'\n",
    ")\n",
    "\n",
    "casos_audit['FLAG_STATE_CHANGE'] = df_incidents[\n",
    "    df_incidents['_IMPROPER_STATE_COUNTS'] > 0\n",
    "]\n",
    "\n",
    "inc_with_childs = df_incidents[df_incidents['CHILD_INCIDENTS'] != 0].NUMBER.unique()\n",
    "casos_audit['INC_with_CHILD'] = df_incidents[\n",
    "    df_incidents['NUMBER'].isin(inc_with_childs)\n",
    "]\n",
    "\n",
    "casos_audit['INC_CompanyChanges'] = (\n",
    "    df_incidents.groupby('NUMBER')['COMPANY']\n",
    "    .agg([pd.Series.nunique, set])\n",
    "    .query('nunique > 1')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_assignment_groups = load_confluence()\n",
    "\n",
    "df_incidents = df_incidents.merge(\n",
    "    df_assignment_groups,\n",
    "    on='LAST_ASSIGNMENT_GROUP',\n",
    "    how='left',\n",
    "    suffixes=('', '_CONFLUENCE'),\n",
    "    indicator='_mergeAG',\n",
    ")\n",
    "df_created_incidents = df_created_incidents.merge(\n",
    "    (\n",
    "        df_assignment_groups.rename(\n",
    "            columns={'LAST_ASSIGNMENT_GROUP': 'ASSIGNMENT_GROUP'}\n",
    "        )\n",
    "    ),\n",
    "    on='ASSIGNMENT_GROUP',\n",
    "    how='left',\n",
    "    suffixes=('', '_CONFLUENCE'),\n",
    "    indicator='_mergeAG',\n",
    ")\n",
    "\n",
    "casos_audit['ASSIG_GROUP_NO_CONFLUENCE'] = df_incidents.query(\n",
    "    '_mergeAG == \"left_only\"'\n",
    ").drop_duplicates(keep='first')\n",
    "\n",
    "casos_audit['ASSIG_GROUP_PBI_NO_CONFLUENCE'] = df_created_incidents.query(\n",
    "    '_mergeAG == \"left_only\"'\n",
    ").drop_duplicates(keep='first')\n",
    "\n",
    "try:\n",
    "    del df_assignment_groups\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_changes['8_Confluence'] = print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f26135",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    df_incidents['REASSIGNMENT_COUNT']\n",
    "    != df_incidents['_ASSIGNMENT_GROUP_COUNT_UNIQUE_PER_INC']\n",
    ")\n",
    "casos_audit['INC_assigment_flags'] = (\n",
    "    df_incidents[mask][\n",
    "        [\n",
    "            'NUMBER',\n",
    "            'UPDATED',\n",
    "            'LAST_ASSIGNMENT_GROUP',\n",
    "            'ASSIGNMENT_GROUP',\n",
    "            '_LAST_ASSIGNMENT_GROUP_COUNT_UNIQUE_PER_INC',\n",
    "            '_ASSIGNMENT_GROUP_COUNT_UNIQUE_PER_INC',\n",
    "            'REASSIGNMENT_COUNT',\n",
    "            '_INC_COUNT_ROWS',\n",
    "        ]\n",
    "    ]\n",
    "    .pipe(sort_values_of_incidents)\n",
    "    .loc[\n",
    "        lambda x: x['REASSIGNMENT_COUNT']\n",
    "        != x['_ASSIGNMENT_GROUP_COUNT_UNIQUE_PER_INC'].map({0: 0, 1: 0})\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_separator(\n",
    "    title=\"Section: [TRANSFORM INCIDENTS DF UNIQUE NUMBER (COMPILE UPDATES)] Description: [Data incident transformation from multiple rows of tickets with updates to a single row]\")\n",
    "load_from_pickle = False\n",
    "try:\n",
    "    log_separator(\"TABLE: 'df_incidents'\", 3)\n",
    "    pickle_path = os.path.join(intermediate_data, \"Data_INC_uniques.pkl\")\n",
    "    if os.path.exists(pickle_path) and load_from_pickle:\n",
    "        logger.info(\"Loading Data_INC_uniques from pickle\")\n",
    "        df_incidents_grouped = pd.read_pickle(pickle_path)\n",
    "    else:\n",
    "        df_incidents_grouped = make_uniques(\n",
    "            df_incidents, principal_col='NUMBER', link_to_column='PROBLEM')\n",
    "        logger.info(\"Saving Data_INC_uniques to pickle\")\n",
    "        df_incidents_grouped.to_pickle(pickle_path)\n",
    "    \n",
    "    df_incidents_grouped = df_incidents_grouped.pipe(create_dt_time_wvariants)\n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    function_name = \"MAIN TRANSFORM INCIDENTS DF UNIQUE NUMBER (COMPILE UPDATES)\"\n",
    "    message = f\"[ERROR]: {function_name} {e}\"\n",
    "    print(message)\n",
    "    logger.error(message)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725bc0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filtrar hasta despues de haber cerado df_incidents_grouped\n",
    "# # exclude NUMBER where CAUSE_CODE == 'Works as designed ' \n",
    "# df_incidents_grouped = df_incidents_grouped[~df_incidents_grouped['CAUSE_CODE'].str.contains('Works as designed', na=False)]\n",
    "\n",
    "# # exclude NUMBER where valor '.BUS.'\n",
    "# # en campos ['LAST_ASSIGNMENT_GROUP','ASSIGNMENT_GROUP','CREATOR_GROUP']\n",
    "# for col in ['LAST_ASSIGNMENT_GROUP','ASSIGNMENT_GROUP','CREATOR_GROUP']:\n",
    "#     df_incidents_grouped = df_incidents_grouped[~df_incidents_grouped[col].str.contains('.BUS.', na=False)]\n",
    "\n",
    "# # exclude\n",
    "# exclude = ['ALLIANZ TECHNOLOGY',\n",
    "#  'ALLIANZ TECHNOLOGY EXTERNAL',\n",
    "#  'ALLIANZ TECHNOLOGY IBEROLATAM BRANCH',\n",
    "#  'ALLIANZ TECHNOLOGY IBEROLATAM EXTERNAL',\n",
    "#  'ALLIANZ TECHNOLOGY GDN INTERNAL']\n",
    "# df_incidents_grouped = df_incidents_grouped[~df_incidents_grouped['COMPANY'].isin(exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_changes['9_Make_Unique'] = print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a345d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define masks based on conditions\n",
    "mask_1 = df_incidents_grouped['PARENT_INCIDENT'] != ''\n",
    "mask_2 = df_incidents_grouped['FCR'] == True\n",
    "\n",
    "# strings = ['GLOB', 'SERVICEDESK', '_SD_', '.SD.', 'WORKPL', 'WPS', 'WORK_PL', 'WORK.PL']\n",
    "# mask_3 = (\n",
    "#     df_incidents_grouped['LAST_ASSIGNMENT_GROUP']\n",
    "#     .str.upper()\n",
    "#     .str.contains('|'.join(strings), na=False)\n",
    "# )\n",
    "\n",
    "# Create new columns based on the masks\n",
    "df_incidents_grouped.loc[mask_1, '_NOT_CHILD'] = True\n",
    "df_incidents_grouped.loc[~mask_1, '_NOT_CHILD'] = False\n",
    "\n",
    "df_incidents_grouped.loc[mask_2, '_NOT_FCR'] = True\n",
    "df_incidents_grouped.loc[~mask_2, '_NOT_FCR'] = False\n",
    "\n",
    "# df_incidents_grouped['_ASSIGNMENT_Glob_SerDes_WorPla'] = False\n",
    "# df_incidents_grouped.loc[mask_3, '_ASSIGNMENT_Glob_SerDes_WorPla'] = True\n",
    "\n",
    "df_incidents_grouped['_FILTER_OUT'] = df_incidents_grouped[\n",
    "    [\n",
    "        '_NOT_CHILD', \n",
    "        '_NOT_FCR', \n",
    "        # '_ASSIGNMENT_Glob_SerDes_WorPla'\n",
    "    ]\n",
    "].any(axis=1)\n",
    "\n",
    "df_incidents_grouped_lost = df_incidents_grouped[df_incidents_grouped['_FILTER_OUT']]\n",
    "df_incidents_grouped = df_incidents_grouped[~df_incidents_grouped['_FILTER_OUT']]\n",
    "\n",
    "mask = df_problem['CREATED_OUT_OF_INCIDENT'].isin(\n",
    "    df_incidents_grouped['NUMBER']\n",
    ") | df_problem['NUMBER'].isin(df_incidents_grouped['PROBLEM'])\n",
    "df_problem_lost = df_problem[~mask]\n",
    "df_problem = df_problem[mask]\n",
    "\n",
    "mask = df_problem_tasks['PROBLEM'].isin(\n",
    "    df_incidents_grouped['PROBLEM']\n",
    ") | df_problem_tasks['PROBLEM'].isin(df_problem['NUMBER'])\n",
    "df_problem_tasks_lost = df_problem_tasks[~mask]\n",
    "df_problem_tasks = df_problem_tasks[mask]\n",
    "\n",
    "try:\n",
    "    del df_problem_tasks_lost, df_problem_lost, df_incidents_grouped_lost\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_changes['10_FiltroParentFCR'] = print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allianz_blue = '#0055A4'\n",
    "# allianz_light_blue = '#A3C7E4'\n",
    "# allianz_gray = '#6C757D'\n",
    "# for col in [col for col in df_incidents_grouped.select_dtypes(exclude=['number','datetime']).columns if df_incidents_grouped[col].nunique() <= 10]:\n",
    "#     plt.figure(figsize=(10, 5))  # Set figure size for better visibility\n",
    "#     df_incidents_grouped[col].value_counts(normalize=True, dropna=False).pipe(lambda ser: ser[ser>0]).plot.barh(\n",
    "#         color=allianz_blue, edgecolor=allianz_light_blue\n",
    "#     )\n",
    "#     plt.title(f'Distribution of {col}', fontsize=14, fontweight='bold', color=allianz_gray)\n",
    "#     plt.xlabel('Frequency', fontsize=12, color=allianz_gray)\n",
    "#     plt.ylabel('Values', fontsize=12, color=allianz_gray)\n",
    "#     plt.grid(axis='x', linestyle='--', linewidth=0.7, color=allianz_light_blue)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()  # Show plot for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7077d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in [col for col in df_incidents_grouped.select_dtypes(exclude=['number', 'datetime']).columns \n",
    "#             if df_incidents_grouped[col].nunique() >= 10 and df_incidents_grouped[col].nunique() <= 30]:\n",
    "#     plt.figure(figsize=(10, 5))  # Set figure size for better visibility\n",
    "#     df_incidents_grouped[col].value_counts(normalize=True, dropna=False).pipe(lambda ser: ser[ser>0]).plot.bar(\n",
    "#         color=allianz_blue, edgecolor=allianz_light_blue\n",
    "#     )\n",
    "#     plt.title(f'Distribution of {col}', fontsize=14, fontweight='bold', color=allianz_gray)\n",
    "#     plt.xlabel('Frequency', fontsize=12, color=allianz_gray)\n",
    "#     plt.ylabel('Values', fontsize=12, color=allianz_gray)\n",
    "#     plt.grid(axis='x', linestyle='--', linewidth=0.7, color=allianz_light_blue)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()  # Show plot for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d2616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # kde o density\n",
    "# for col in [col for col in df_incidents_grouped.select_dtypes(include=['number']).columns]:\n",
    "#     plt.figure(figsize=(10, 5))  # Set figure size for better visibility\n",
    "#     df_incidents_grouped[col].plot.hist(\n",
    "#         bins=25,\n",
    "#         color=allianz_blue, \n",
    "#         #edgecolor=allianz_light_blue\n",
    "#     )\n",
    "#     plt.title(f'Distribution of {col}', fontsize=14, fontweight='bold', color=allianz_gray)\n",
    "#     plt.xlabel('Frequency', fontsize=12, color=allianz_gray)\n",
    "#     plt.ylabel('Values', fontsize=12, color=allianz_gray)\n",
    "#     plt.grid(axis='x', linestyle='--', linewidth=0.7, color=allianz_light_blue)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()  # Show plot for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23deaed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_pickle = False\n",
    "pickle_path = os.path.join(intermediate_data, \"df_incidents_grouped_pnc.pkl\")\n",
    "if os.path.exists(pickle_path) and load_from_pickle:\n",
    "    logger.info(\"Loading df_incidents_grouped from pickle\")\n",
    "    df_incidents_grouped = pd.read_pickle(pickle_path)\n",
    "    df_problem = pd.read_pickle(os.path.join(intermediate_data, \"df_problem_pnc.pkl\"))\n",
    "    df_problem_tasks = pd.read_pickle(\n",
    "        os.path.join(intermediate_data, \"df_problem_tasks_pnc.pkl\")\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Saving df_incidents_grouped to pickle\")\n",
    "    # INCIDENTS\n",
    "    date_pairs = time_cols_pairs['df_incidents']\n",
    "    df_incidents_grouped = process_not_closed_business_vectorized(\n",
    "        df_incidents_grouped, date_pairs\n",
    "    )\n",
    "    df_incidents_grouped.to_pickle(pickle_path)\n",
    "\n",
    "    # PROBLEMS\n",
    "    date_pairs = time_cols_pairs['df_problem']\n",
    "    df_problem = process_not_closed_business_vectorized(df_problem, date_pairs)\n",
    "    df_problem.to_pickle(os.path.join(intermediate_data, \"df_problem_pnc.pkl\"))\n",
    "\n",
    "    # PROBLEM TASKS\n",
    "    date_pairs = time_cols_pairs['df_problem_tasks']\n",
    "    df_problem_tasks = process_not_closed_business_vectorized(\n",
    "        df_problem_tasks, date_pairs\n",
    "    )\n",
    "    df_problem_tasks.to_pickle(\n",
    "        os.path.join(intermediate_data, \"df_problem_tasks_pnc.pkl\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9351e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_changes['11_NotClosed'] = print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_grouped['COMPANY'] = df_incidents_grouped['COMPANY'].apply(\n",
    "    standardize_company_name\n",
    ")\n",
    "df_incidents_grouped['_CLOSE_NOTES'] = clean_close_notes(\n",
    "    df_incidents_grouped['CLOSE_NOTES']\n",
    ")\n",
    "df_incidents_grouped['_nullProblem'] = df_incidents_grouped['PROBLEM'].isna()\n",
    "\n",
    "df_incidents_grouped = exceed_bands_priority_CRE_RES_optimized(\n",
    "    df_incidents_grouped, 'CURRENT_PRIORITY'\n",
    ")  # PRIORITY | HIGHEST_PRIORITY\n",
    "\n",
    "# PROBLEMS\n",
    "df_problem = exceed_bands_priority_CRE_RES_optimized(df_problem, 'CURRENT_PRIORITY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ce188",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp2 = ['INC29879295','INC31431639','INC31422945']\n",
    "casos_audit['2025_CP\"'] = df_incidents[df_incidents['NUMBER'].isin(cp2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f53f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_changes['12_Bands_Exceed'] = print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdcf53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f13a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_audit['Priority_ProblemNULL_Year'] = df_incidents_grouped.pivot_table(\n",
    "    index='CURRENT_PRIORITY',\n",
    "    columns='_CREATED_year',\n",
    "    values='_nullProblem',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0,\n",
    "    margins=True,\n",
    "    margins_name='Total',\n",
    ").reset_index()\n",
    "\n",
    "# prueba:\n",
    "# (df_incidents_grouped[(df_incidents_grouped['CURRENT_PRIORITY']=='P3') &\n",
    "#                      (df_incidents_grouped['PROBLEM'].isna()) &\n",
    "#                      (df_incidents_grouped['_CREATED_year']==2025)\n",
    "#                      ]\n",
    "# ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f47a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to days\n",
    "# df_incidents_grouped['_DIFB_CREATED_RESOLVED_days'] = df_incidents_grouped['_DIFB_CREATED_RESOLVED'].dt.total_seconds() / (24 * 60 * 60)\n",
    "\n",
    "# Or convert to hours\n",
    "# df_incidents_grouped['_DIFB_CREATED_RESOLVED_hours'] = df_incidents_grouped['_DIFB_CREATED_RESOLVED'].dt.total_seconds() / 3600\n",
    "\n",
    "# Or convert to seconds\n",
    "# df_incidents_grouped['_DIFB_CREATED_RESOLVED_seconds'] = df_incidents_grouped['_DIFB_CREATED_RESOLVED'].dt.total_seconds()\n",
    "\n",
    "# Create pivot table with different aggregations for different columns\n",
    "casos_audit['PT_SLA_GROUPS'] = df_incidents_grouped.pivot_table(\n",
    "    index=[\n",
    "        'CURRENT_PRIORITY',\n",
    "        'STATE',\n",
    "        'LAST_ASSIGNMENT_GROUP',\n",
    "        '_SLA_CURRENT_PRIORITY',\n",
    "        '_SLA_THRESHOLD',\n",
    "    ],\n",
    "    columns='_CREATED_year',\n",
    "    values=['_DIFB_CREATED_RESOLVED', 'NUMBER'],\n",
    "    aggfunc={\n",
    "        '_DIFB_CREATED_RESOLVED': ['mean', 'std', 'min', 'max', 'count'],\n",
    "        'NUMBER': [lambda x: list(x), 'nunique'],\n",
    "    },\n",
    "    fill_value=0,\n",
    ").reset_index()\n",
    "\n",
    "#  margins=True,\n",
    "#  margins_name='Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Campos con valores nulos\n",
    "# casos_audit[\"VALORES_NULOS\"] = df_incidents_grouped.isna().sum().reset_index(name='SUM').query(\"SUM > 0\").assign(_PCT=lambda x: x['SUM'] / df_incidents_grouped.shape[0] * 100)\n",
    "# Se han observado 3 casos AFFECTED_OES en blanco y compaÃ±ia == 'ALLIANZ COMPANIA DE SEGUROS Y REASEGUROS SA'\n",
    "casos_audit[\"AFFECTED_OES_nulos\"] = df_incidents_grouped[\n",
    "    df_incidents_grouped.filter(regex='AFF')['AFFECTED_OES'].isna()\n",
    "]\n",
    "# 4 incidententes con 'CAUSE_CODE' en blanco y 'STATE' = 'Closed'\n",
    "mask = (df_incidents_grouped['CAUSE_CODE'].isna()) & (\n",
    "    df_incidents_grouped['STATE'] == 'Closed'\n",
    ")\n",
    "casos_audit[\"CAUSE_CODE_nulos_Closed\"] = df_incidents_grouped[mask][\n",
    "    ['NUMBER', 'CLOSED', 'CAUSE_CODE', 'STATE']\n",
    "]\n",
    "# Campos con valores nulos en 'CAUSE_CODE'\n",
    "mask = df_incidents_grouped['CAUSE_CODE'].isna()\n",
    "casos_audit[\"CAUSE_CODE_nulos_Campos\"] = append_sample_to_value_counts(\n",
    "    df_incidents_grouped[mask].STATE, df_incidents_grouped, mask\n",
    ").reset_index()\n",
    "# 4 casos 'CAUSE_CODE'=='Other' y 'STATE'=='Closed'\n",
    "mask = (~df_incidents_grouped['CAUSE_CODE'].isna()) & (\n",
    "    df_incidents_grouped['STATE'] == 'Closed'\n",
    ")\n",
    "casos_audit[\"CAUSE_CODE_Other_Closed\"] = (\n",
    "    df_incidents_grouped[mask][['CAUSE_CODE', 'STATE']].value_counts().reset_index()\n",
    ")\n",
    "# casos audit_fail 'CLOSE_NOTES'== y 'STATE'=='Closed'\n",
    "mask = (\n",
    "    (~df_incidents_grouped['_CLOSE_NOTES'].isna())\n",
    "    & (df_incidents_grouped['STATE'] == 'Closed')\n",
    "    & (df_incidents_grouped['_CLOSE_NOTES'].str.len() < 15)\n",
    ")\n",
    "casos_audit[\"CLOSE_NOTES_fails_Closed\"] = (\n",
    "    df_incidents_grouped[mask][['_CLOSE_NOTES', 'STATE']].value_counts().reset_index()\n",
    ")\n",
    "# AnÃ¡lisis de correlaciÃ³n\n",
    "casos_audit[\"corr_ASSIGMENT_SERVICE\"] = (\n",
    "    df_incidents_grouped[['LAST_ASSIGNMENT_GROUP', 'SERVICE_OFFERING']]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "casos_audit[\"corr_SYMPTOM_SERVICE\"] = (\n",
    "    df_incidents_grouped[['SYMPTOM', 'SERVICE_OFFERING']]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "casos_audit[\"corr_ASSIGMENT_SERVICE2\"] = analyze_service_offerings(\n",
    "    df_incidents_grouped, 'LAST_ASSIGNMENT_GROUP', 'SERVICE_OFFERING'\n",
    ")\n",
    "casos_audit[\"corr_SYMPTOM_SERVICE2\"] = analyze_service_offerings(\n",
    "    df_incidents_grouped, 'SYMPTOM', 'SERVICE_OFFERING'\n",
    ")\n",
    "casos_audit[\"corr_SYMPTOM_ASSIGMENT\"] = analyze_service_offerings(\n",
    "    df_incidents_grouped, 'SYMPTOM', 'LAST_ASSIGNMENT_GROUP'\n",
    ")\n",
    "# Aging\n",
    "# casos_audit[\"AGING\"] = df_incidents_grouped.groupby('STATE')[df_incidents_grouped.filter(like='_DIFB').columns].agg(['mean', 'std', 'count']).reset_index()\n",
    "casos_audit[\"AGING_Business\"] = (\n",
    "    df_incidents_grouped.groupby(['STATE', 'CURRENT_PRIORITY'])[\n",
    "        df_incidents_grouped.filter(like='_DIFB').columns\n",
    "    ]\n",
    "    .agg(['mean', 'std', 'count', 'min', 'max'])\n",
    "    .reset_index()\n",
    ")\n",
    "# Incident tickets PRIORITY vs OUTAGE_BEGIN to CLOSED\n",
    "# casos_audit[\"PRIORITY_OUTAGE_BEGIN_CLOSED\"] = (\n",
    "#     df_incidents_grouped.loc[df_incidents_grouped['STATE'] == 'Closed']\n",
    "#     .groupby(['CURRENT_PRIORITY', 'STATE'])['_DIFB_CREATED_RESOLVED']\n",
    "#     .agg(['mean', 'std', 'count', 'min', 'max'])\n",
    "#     .reset_index()\n",
    "# )\n",
    "\n",
    "# Problems root_couse\n",
    "dfs = []\n",
    "for col in [\n",
    "    'CURRENT_SYMPTOM',\n",
    "    'SUBCLASSIFICATION',\n",
    "    'CLASSIFICATION',\n",
    "    'ROOT_CAUSE_KNOWN',\n",
    "    'CLOSE_CODE',\n",
    "]:\n",
    "    value_counts = (\n",
    "        df_problem.groupby('STATE')[col]\n",
    "        .value_counts(dropna=False)\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "    value_counts['_PCT'] = (\n",
    "        value_counts['count']\n",
    "        / value_counts.groupby('STATE')['count'].transform('sum')\n",
    "        * 100\n",
    "    )\n",
    "    value_counts['_COL'] = col\n",
    "    value_counts.columns = ['STATE', '_VALUE', '_COUNT', '_PCT', '_COL_GROUP']\n",
    "    dfs.append(value_counts)\n",
    "df_problems_root_cause = pd.concat(dfs, ignore_index=True)\n",
    "del dfs\n",
    "\n",
    "casos_audit[\"PROBLEMS_ROOT_CAUSE\"] = df_problems_root_cause\n",
    "# casos_audit[\"INCIDENTS_LOST\"] = df_incidents_grouped_lost\n",
    "# casos_audit[\"PROBLEMS_LOST\"] = df_problem_lost\n",
    "# casos_audit[\"PROBLEMS_TASK_LOST\"] = df_problem_tasks_lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_grouped[['LAST_ASSIGNMENT_GROUP', 'SERVICE_OFFERING','_SERVICE_OFFERING_OE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OUTAGE_BEGIN in STAGES Closed|Resolved\n",
    "mask_1 = df_incidents_grouped['OUTAGE_BEGIN'].isna() & (df_incidents_grouped['STATE'] == 'Closed')\n",
    "mask_2 = df_incidents_grouped['OUTAGE_BEGIN'].isna() & (df_incidents_grouped['STATE'] == 'Resolved')\n",
    "combined = mask_1 | mask_2\n",
    "# df_incidents_grouped[combined]\n",
    "df_incidents_grouped['_OutBegNull_StateCloRes'] = combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = df_incidents_grouped.isna().sum()\n",
    "missing_values_pct = df_incidents_grouped.isna().mean() * 100\n",
    "missing_values_summary = pd.DataFrame({\n",
    "    'count': missing_values_count,\n",
    "    'percentage': missing_values_pct\n",
    "})\n",
    "columns_with_missing_values = missing_values_summary[missing_values_summary['count'] > 0]\n",
    "sorted_columns = columns_with_missing_values.sort_values(by='count')\n",
    "casos_audit['VALORES_NULOS_INC'] = sorted_columns.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a99f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = df_problem.isna().sum()\n",
    "missing_values_pct = df_problem.isna().mean() * 100\n",
    "missing_values_summary = pd.DataFrame({\n",
    "    'count': missing_values_count,\n",
    "    'percentage': missing_values_pct\n",
    "})\n",
    "columns_with_missing_values = missing_values_summary[missing_values_summary['count'] > 0]\n",
    "sorted_columns = columns_with_missing_values.sort_values(by='count')\n",
    "casos_audit['VALORES_NULOS_PRB'] = sorted_columns.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5129e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c71627",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_separator(title=\"Section: [TEST POWERBI DFS VS SERVICE NOW DFS] Description: [Add PowerBI ticket numbers comparison]\")\n",
    "try:\n",
    "    # PowerBI dfs VS ServiceNow dfs\n",
    "    # df_incidents_grouped = df_incidents_grouped.merge(df_created_incidents[['NUMBER']], on=['NUMBER'], how='left', suffixes=('', '_CRE_PBI'), indicator='_merge_CRE_PBI')\n",
    "    # df_incidents_grouped = df_incidents_grouped.merge(df_resolved_incidents[['NUMBER']], on=['NUMBER'], how='left', suffixes=('', '_RES_PBI'), indicator='_merge_RES_PBI')\n",
    "    # df_problem = df_problem.merge(df_problems_pbi[['NUMBER']], on=['NUMBER'], how='left', suffixes=('', '_PRB_PBI'), indicator='_merge_PRB_PBI')\n",
    "    # # ------------------------------------------------------------\n",
    "    # Extract unique incident numbers from different DataFrames\n",
    "    powerbi_cre_inc_num = df_created_incidents['NUMBER'].dropna().unique()\n",
    "    powerbi_cre_prb_num = df_created_incidents['PROBLEM'].dropna().unique()\n",
    "    \n",
    "    powerbi_res_inc_num = df_resolved_incidents['NUMBER'].dropna().unique()\n",
    "    \n",
    "    powerbi_prb_prb_num = df_problems_pbi['NUMBER'].dropna().unique()\n",
    "    powerbi_prb_inc_num = df_problems_pbi['CREATED_OUT_OF_INCIDENT'].dropna().unique()\n",
    "    # ------------------------------------------------------------\n",
    "    # Create dictionaries for mapping\n",
    "    powerbi_cre_inc_num = {num: num for num in powerbi_cre_inc_num}\n",
    "    powerbi_cre_prb_num = {num: num for num in powerbi_cre_prb_num}\n",
    "    \n",
    "    powerbi_res_inc_num = {num: num for num in powerbi_res_inc_num}\n",
    "    \n",
    "    powerbi_prb_prb_num = {num: num for num in powerbi_prb_prb_num}\n",
    "    powerbi_prb_inc_num = {num: num for num in powerbi_prb_inc_num}\n",
    "    # ------------------------------------------------------------\n",
    "    # Check if incident numbers are in the respective lists and log the results\n",
    "    df_incidents_grouped[\"_CRE_INC_Match\"] = df_incidents_grouped[\"NUMBER\"].map(powerbi_cre_inc_num)\n",
    "    df_incidents_grouped[\"_CRE_PRB_Match\"] = df_incidents_grouped[\"PROBLEM\"].map(powerbi_cre_prb_num)\n",
    "    \n",
    "    df_incidents_grouped[\"_RES_INC_Match\"] = df_incidents_grouped[\"NUMBER\"].map(powerbi_res_inc_num)\n",
    "    \n",
    "    df_incidents_grouped[\"_PRB_PRB_Match\"] = df_incidents_grouped[\"PROBLEM\"].map(powerbi_prb_prb_num)\n",
    "    df_incidents_grouped[\"_PRB_INC_Match\"] = df_incidents_grouped[\"NUMBER\"].map(powerbi_prb_inc_num)\n",
    "    # ------------------------------------------------------------\n",
    "    df_problem[\"_PRB_PRB_Match\"] = df_problem[\"NUMBER\"].map(powerbi_prb_prb_num)\n",
    "    df_problem[\"_PRB_INC_Match\"] = df_problem[\"NUMBER\"].map(powerbi_prb_inc_num)\n",
    "    \n",
    "    df_problem[\"_CRE_INC_Match\"] = df_problem[\"CREATED_OUT_OF_INCIDENT\"].map(powerbi_cre_inc_num)\n",
    "    df_problem[\"_CRE_PRB_Match\"] = df_problem[\"NUMBER\"].map(powerbi_cre_prb_num)\n",
    "    \n",
    "    df_problem[\"_RES_INC_Match\"] = df_problem[\"CREATED_OUT_OF_INCIDENT\"].map(powerbi_res_inc_num)\n",
    "    # ------------------------------------------------------------\n",
    "    df_problem_tasks[\"_PRB_PRB_Match\"] = df_problem_tasks[\"PROBLEM\"].map(powerbi_prb_prb_num)\n",
    "    df_problem_tasks[\"_CRE_PRB_Match\"] = df_problem_tasks[\"PROBLEM\"].map(powerbi_cre_prb_num)\n",
    "    # ------------------------------------------------------------\n",
    "    # Initialize result DataFrame\n",
    "    df_result = pd.DataFrame()\n",
    "    # Append matches to result DataFrame\n",
    "    df_result = pd.concat([df_result, (df_incidents_grouped\n",
    "                                       [['NUMBER', 'PROBLEM', '_CRE_INC_Match', '_CRE_PRB_Match', '_RES_INC_Match', '_PRB_PRB_Match', '_PRB_INC_Match']+df_incidents_grouped.filter(like=\"_PBI\").columns.to_list()]\n",
    "                                       .assign(Source='df_incidents_grouped')\n",
    "                                       .rename(columns={'NUMBER': 'Incident Number', \n",
    "                                                        'PROBLEM': 'Problem Number'})\n",
    "                                       )])\n",
    "    # Append matches to result DataFrame\n",
    "    df_result = pd.concat([df_result, (df_problem\n",
    "                                       [['NUMBER', 'CREATED_OUT_OF_INCIDENT', '_CRE_INC_Match', '_CRE_PRB_Match', '_RES_INC_Match', '_PRB_PRB_Match', '_PRB_INC_Match']+df_problem.filter(like=\"_PBI\").columns.to_list()]\n",
    "                                       .assign(Source='df_problem')\n",
    "                                       .rename(columns={'NUMBER':'Problem Number', \n",
    "                                                        'CREATED_OUT_OF_INCIDENT':'Incident Number'})\n",
    "                                       )])\n",
    "    # Append matches to result DataFrame\n",
    "    df_result = pd.concat([df_result, (df_problem_tasks\n",
    "                                       [['NUMBER', 'PROBLEM', '_PRB_PRB_Match', '_CRE_PRB_Match']]\n",
    "                                       .assign(Source='df_problem_tasks')\n",
    "                                       .rename(columns={'NUMBER':'Problem Task Number', \n",
    "                                                        'PROBLEM':'Problem Number'})\n",
    "                                       )])\n",
    "    cols = ['Source', 'Incident Number', 'Problem Number', 'Problem Task Number']\n",
    "    df_result = df_result.pipe(set_df_column_order, start_cols=cols).pipe(clean_headers)\n",
    "    display(df_result)\n",
    "    casos_audit[\"vs_PBI\"] = df_result\n",
    "\n",
    "except Exception as e:\n",
    "    function_name = \"MAIN TEST POWERBI DFS VS SERVICE NOW DFS\"\n",
    "    message = f\"[ERROR]: {function_name} {e}\"\n",
    "    print(message)\n",
    "    logger.error(message)\n",
    "    raise e\n",
    "\n",
    "log_separator(title=\"Section: [MAIN INFO OF CHANGES] Description: [DataFrame changes (checkpoint)]\", level=3)\n",
    "try:\n",
    "    transformed_columns = {\n",
    "        \"df_incidents\": df_incidents.columns.tolist(),\n",
    "        \"df_problem\": df_problem.columns.tolist(),\n",
    "        \"df_problem_tasks\": df_problem_tasks.columns.tolist()\n",
    "    }\n",
    "    # ------------------------------------------------------------\n",
    "    transformed_shapes = {\n",
    "        \"df_incidents\": df_incidents.shape,\n",
    "        \"df_problem\": df_problem.shape,\n",
    "        \"df_problem_tasks\": df_problem_tasks.shape\n",
    "    }\n",
    "    # ------------------------------------------------------------\n",
    "    message=\"On Section: [INFO OF CHANGES]\"\n",
    "    dict_transformed_cols = compare_cols_dicts(original_columns, transformed_columns, message=message)\n",
    "    dict_transformed_shapes = compare_shapes_dicts(original_shapes, transformed_shapes, message=message)\n",
    "    print(dict_transformed_cols)\n",
    "    print(dict_transformed_shapes)\n",
    "    \n",
    "except Exception as e:\n",
    "    function_name = \"MAIN INFO OF CHANGES\"\n",
    "    message = f\"[ERROR]: {function_name} {e}\"\n",
    "    print(message)\n",
    "    logger.error(message)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac73ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_those_with_more_than_one_coma(series):\n",
    "#     return series[series.str.count(',') > 1]\n",
    "# df_incidents_grouped._CHANGES_IN_LINK.pipe(filter_those_with_more_than_one_coma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad532305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISS09\n",
    "casos_audit['PTASK_TYPE_pt_year'] = (df_problem_tasks\n",
    "                                     .pivot_table(\n",
    "    index=['TYPE_OF_TASK'],\n",
    "    columns='_CREATED_year',\n",
    "    values='NUMBER',\n",
    "    aggfunc='count',\n",
    "    dropna=True\n",
    ")\n",
    "    .assign(TOTAL=lambda x: x[x.select_dtypes(include='number').columns].sum(axis=1))\n",
    "    .sort_values(by='TOTAL', ascending=False)\n",
    "    .assign(PCT=lambda x: 100 * np.round(x.TOTAL / x.TOTAL.sum(), 4))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_problem_tasks_Lessons = (\n",
    "    df_problem_tasks[['NUMBER','CLOSE_NOTES','TYPE_OF_TASK','_CREATED_year','SHORT_DESCRIPTION']]\n",
    ")\n",
    "\n",
    "# tabulate_df(df_problem_tasks_Lessons[df_problem_tasks_Lessons['_CREATED_year'] == 2025])\n",
    "display(df_problem_tasks_Lessons[df_problem_tasks_Lessons['_CREATED_year'] == 2025])\n",
    "\n",
    "grouped_tasks_2025 = (df_problem_tasks_Lessons[df_problem_tasks_Lessons['_CREATED_year'] == 2025]\n",
    "               .groupby('TYPE_OF_TASK').agg(\n",
    "                   Total_Tasks=('NUMBER', 'count'),\n",
    "                   Empty_CLOSE_NOTES=('CLOSE_NOTES', lambda x: x.isna().sum())\n",
    "               ).sort_values(by='Total_Tasks', ascending=False)\n",
    ")\n",
    "\n",
    "# Calcular el porcentaje de tareas con CLOSE_NOTES vacÃ­as\n",
    "grouped_tasks_2025['Percentage_Empty_CLOSE_NOTES'] = (\n",
    "    grouped_tasks_2025['Empty_CLOSE_NOTES'] / grouped_tasks_2025['Total_Tasks'] * 100\n",
    ")\n",
    "\n",
    "# Calcular el porcentaje de cada TYPE_OF_TASK respecto al total de tareas en 2025\n",
    "total_tasks_2025 = grouped_tasks_2025['Total_Tasks'].sum()\n",
    "grouped_tasks_2025['Percentage_TYPE_OF_TASK'] = (\n",
    "    grouped_tasks_2025['Total_Tasks'] / total_tasks_2025 * 100\n",
    ")\n",
    "\n",
    "# Formatear los porcentajes para mostrar como valores con dos decimales\n",
    "grouped_tasks_2025['Percentage_Empty_CLOSE_NOTES'] = grouped_tasks_2025['Percentage_Empty_CLOSE_NOTES'].map('{:.2f}%'.format)\n",
    "grouped_tasks_2025['Percentage_TYPE_OF_TASK'] = grouped_tasks_2025['Percentage_TYPE_OF_TASK'].map('{:.2f}%'.format)\n",
    "grouped_tasks_2025.reset_index()\n",
    "\n",
    "# tabulate_df(grouped_tasks_2025)\n",
    "display(grouped_tasks_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dates_periods(df_incidents_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    'LESSON','LECCION', 'LECCIÃ“N',\n",
    "    # 'ROOT CAUSE', 'CAUSA RAIZ'\n",
    "]\n",
    "\n",
    "df_incidents_grouped = df_incidents_grouped.assign(_word_Lesson= lambda x: x.apply(find_words_in_row, words=keywords, axis=1))\n",
    "\n",
    "keywords = [\n",
    "    # 'LESSON','LECCION', 'LECCIÃ“N',\n",
    "    'ROOT CAUSE', 'CAUSA RAIZ'\n",
    "]\n",
    "\n",
    "df_incidents_grouped = df_incidents_grouped.assign(_word_RootCause= lambda x: x.apply(find_words_in_row, words=keywords, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(\"process_dataframe: df_problem\")\n",
    "process_dataframe_stage(df_problem, ['CURRENT_PRIORITY'], examples=True)\n",
    "\n",
    "# logger.info(\"process_dataframe: df_problem_tasks\")\n",
    "# process_dataframe(df_problem_tasks, ['TYPE_OF_TASK'], examples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cols = ['ASSIGNMENT_GROUP', 'AFFECTED_OES', 'PRIORITY','STATE','CREATED','RESOLVED','CLOSE_NOTES','ROOT_CAUSE_KNOWN','PROBLEM_TASKS','CLASSIFICATION']\n",
    "result = find_columns_value_match(df_problem)[['NUMBER','CREATED_OUT_OF_INCIDENT','_LINKED_TICKETS']]\n",
    "\n",
    "\n",
    "result = get_LINKED_TICKETS_summary(result)\n",
    "display(result)\n",
    "casos_audit[\"LINKED_TICKETS_PBR\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cff7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['ASSIGNMENT_GROUP', 'AFFECTED_OES', 'PRIORITY','STATE','CREATED','RESOLVED','CLOSE_NOTES','ROOT_CAUSE_KNOWN','PROBLEM_TASKS','CLASSIFICATION']\n",
    "result = find_columns_value_match(df_incidents_grouped)[['NUMBER','PROBLEM','_LINKED_TICKETS']]\n",
    "\n",
    "result = get_LINKED_TICKETS_summary(result)\n",
    "display(result)\n",
    "casos_audit[\"LINKED_TICKETS_INC\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_grouped = find_columns_value_match(df_incidents_grouped, \n",
    "                         ignore_columns=['NUMBER', 'CREATED_OUT_OF_INCIDENT', 'PROBLEM', '_PROBLEM_LIST_PER_INC','_CHANGES_DICT', '_CRE_INC_Match', \n",
    "                                         '_CRE_PRB_Match', '_RES_INC_Match', '_PRB_PRB_Match', '_PRB_INC_Match','_values_NUMBER','_values_PROBLEM']\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b084e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192aabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_incidents_grouped.query('_LINKED_TICKETS_COUNT > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplos_problems = pd.read_excel(raw_data+'/EJEMPLOS_QA.xlsx').PROBLEM_NUMBER.to_list()\n",
    "# print(ejemplos_problems)\n",
    "# ejemplos_incidents = df_problem.loc[lambda x: x['NUMBER'].isin(ejemplos_problems)].CREATED_OUT_OF_INCIDENT.unique()\n",
    "# print(ejemplos_incidents)\n",
    "\n",
    "# # save filtered DataFrames to Excel\n",
    "# with pd.ExcelWriter(processed_data + '/filtered_examples.xlsx') as writer:\n",
    "#     df_problem.loc[lambda x: x['NUMBER'].isin(ejemplos_problems)].to_excel(writer, sheet_name='Filtered_Problems', index=False)\n",
    "#     df_incidents.loc[lambda x: x['NUMBER'].isin(ejemplos_incidents)].to_excel(writer, sheet_name='Filtered_Incidents', index=False)\n",
    "#     df_problem_tasks.loc[lambda x: x['PROBLEM'].isin(ejemplos_problems)].to_excel(writer, sheet_name='Filtered_Problem_Tasks', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e78f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IncidentAnalyzer:\n",
    "    def __init__(self, df, config):\n",
    "        self.df = df\n",
    "        self.config = config\n",
    "\n",
    "    def check_duplicates(self, column_name):\n",
    "        logger.info(f\"Config keys used: {column_name}, problem_column\")\n",
    "        if self.df[self.config[column_name]].duplicated().any():\n",
    "            logger.warning(f\"Duplicate {self.config[column_name]} found in {column_name}.\")\n",
    "            if self.config[column_name] == self.config['problem_column']:\n",
    "                dup_df = (self.df[lambda x: x[self.config[column_name]].duplicated(keep=False)]\n",
    "                          .groupby(self.config[column_name])[self.config['number_column']]\n",
    "                            .agg(\n",
    "                                nunique=lambda x: x.nunique(dropna=False),\n",
    "                                values=lambda x: ','.join(set(val for val in x if pd.notnull(val))),\n",
    "                                contains_null=lambda x: x.isnull().any()\n",
    "                            )\n",
    "                          .reset_index()\n",
    "                          )\n",
    "                logger.warning(\"\\n\" + tabulate(dup_df, headers='keys', tablefmt='psql'))\n",
    "            if self.config[column_name] == self.config['number_column']:\n",
    "                dup_df = (self.df[lambda x: x[self.config[column_name]]\n",
    "                                  .duplicated(keep=False)]\n",
    "                          .groupby(self.config[column_name])[self.config['problem_column']]\n",
    "                            .agg(\n",
    "                                nunique=lambda x: x.nunique(dropna=False),\n",
    "                                values=lambda x: ','.join(set(val for val in x if pd.notnull(val))),\n",
    "                                contains_null=lambda x: x.isnull().any()\n",
    "                            )\n",
    "                          .reset_index()\n",
    "                          )\n",
    "                logger.warning(\"\\n\" + tabulate(dup_df, headers='keys', tablefmt='psql'))\n",
    "        else:\n",
    "            logger.info(f\"No duplicate {self.config[column_name]} found in {column_name}.\")\n",
    "            unique_items = self.df[self.config[column_name]].unique().tolist()\n",
    "            logger.info(f\"Unique {self.config[column_name]}: {len(unique_items)}\")\n",
    "    \n",
    "    def check_missing(self, column_name):\n",
    "        logger.info(f\"Config keys used: {column_name}\")\n",
    "        if self.df[self.config[column_name]].isna().any():\n",
    "            missing_count = self.df[self.config[column_name]].isna().sum()\n",
    "            total_count = len(self.df)\n",
    "            ratio = missing_count / total_count if total_count > 0 else 0\n",
    "            logger.warning(\n",
    "                f\"Missing {self.config[column_name]} found in {column_name}. \"\n",
    "                f\"Count: {missing_count}, Ratio: {missing_count}/{total_count} = {ratio:.4f} ({ratio:.2%})\"\n",
    "            )\n",
    "        else:\n",
    "            logger.info(f\"No missing {self.config[column_name]} found in {column_name}.\")\n",
    "\n",
    "    def check_priority_problems(self, priority_list, has_problem=True):\n",
    "        logger.info(f\"Config keys used: {priority_list}, problem_column, priority_column\")\n",
    "        condition = self.df[self.config['priority_column']].isin(self.config[priority_list]) & self.df[self.config['problem_column']].isna()\n",
    "        if has_problem:\n",
    "            condition = ~condition\n",
    "        if self.df[condition].empty:\n",
    "            logger.info(f\"All {self.config[priority_list]} priority cases with {'no ' if not has_problem else ''}linking tickets found.\")\n",
    "        else:\n",
    "            logger.warning(f\"Some {self.config[priority_list]} priority cases with {'no ' if not has_problem else ''}linking tickets found.\")\n",
    "            if not has_problem:\n",
    "                total = self.df[self.df[self.config['priority_column']].isin(self.config[priority_list])].shape[0]\n",
    "                no_problem = self.df[condition].shape[0]\n",
    "                if total > 0:\n",
    "                    ratio = no_problem / total\n",
    "                    logger.info(\n",
    "                        f\"Ratio of {self.config[priority_list]} priority cases with no linking tickets: \"\n",
    "                        f\"{no_problem}/{total} = {ratio:.2%} \"\n",
    "                        f\"({no_problem} cases out of {total})\"\n",
    "                    )\n",
    "                else:\n",
    "                    logger.info(f\"No {self.config[priority_list]} priority cases found in created cases.\")\n",
    "\n",
    "    def clean_str_column(self, col_name):\n",
    "        logger.info(f\"Config keys used: string_columns:{col_name}\")\n",
    "        self.df = (\n",
    "            self.df\n",
    "            .assign(**{col_name: lambda x: x[col_name].str.upper().str.strip()})\n",
    "            .assign(**{col_name: lambda x: x[col_name].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')})\n",
    "            .assign(**{col_name: lambda x: x[col_name].str.replace(r'[^A-Z0-9_]', '_', regex=True)})\n",
    "            .assign(**{col_name: lambda x: x[col_name].str.replace(r'_+', '_', regex=True)})\n",
    "            .assign(**{col_name: lambda x: x[col_name].str.replace(r'_$', '', regex=True)})\n",
    "        )\n",
    "\n",
    "    def get_pivot(self, index_columns, columns=None, df=None):\n",
    "        \n",
    "        def changes(pivot, percentage=10, min_change_units=10):\n",
    "            pivot = pivot.iloc[:, :-1]  # Eliminar la columna TOTAL\n",
    "\n",
    "            # Calcular cambios porcentuales sin rellenar NA automÃ¡ticamente\n",
    "            changes = pivot.pct_change(axis=1, fill_method=None) * 100\n",
    "\n",
    "            # Crear una lista para almacenar los cambios mÃ¡s abruptos\n",
    "            abrupt_changes = []\n",
    "\n",
    "            # Recopilar informaciÃ³n sobre los cambios mÃ¡s grandes\n",
    "            for cause in pivot.index:\n",
    "                # analiza si es un index simple o si viene un index tupla. Si es index sencillo seran cambios de aÃ±o de 2023 a 2025, si es tupla primero esta agrupado por un string(categoria) y luego por el aÃ±o\n",
    "                # como en unos aÃ±os puede desaparecer ya que no hubieron casos en esa categoria entonces no se podra hacer comparacion. \n",
    "                for i in range(1, len(pivot.columns)):\n",
    "                    current_period = pivot.columns[i]\n",
    "                    previous_period = pivot.columns[i - 1]\n",
    "\n",
    "                    change = None  # Inicializar la variable change\n",
    "\n",
    "                    # Determinar si las columnas son tuplas o simples aÃ±os\n",
    "                    if isinstance(current_period, tuple) and isinstance(previous_period, tuple):\n",
    "                        # Comparar si el primer elemento es el mismo y el segundo es diferente (para tuplas)\n",
    "                        if current_period[0] == previous_period[0] and current_period[1] > previous_period[1]:\n",
    "                            change = changes.loc[cause, current_period]\n",
    "                    elif isinstance(current_period, int) and isinstance(previous_period, int):\n",
    "                        # Comparar si los aÃ±os son diferentes (para aÃ±os simples)\n",
    "                        if current_period != previous_period:\n",
    "                            change = changes.loc[cause, current_period]\n",
    "\n",
    "                    # Solo proceder si change ha sido asignado, es positivo, y el cambio en unidades es mayor al umbral\n",
    "                    if change is not None and pd.notna(change) and change > 0:\n",
    "                        previous_value = pivot.loc[cause, previous_period]\n",
    "                        current_value = pivot.loc[cause, current_period]\n",
    "                        if pd.notna(previous_value) and pd.notna(current_value) and (current_value - previous_value) > min_change_units:\n",
    "                            abrupt_changes.append((cause, previous_period, current_period, change, previous_value, current_value))\n",
    "                            \n",
    "            # Verificar si hay cambios abruptos antes de proceder\n",
    "            if not abrupt_changes:\n",
    "                logger.info(\"No se encontraron cambios abruptos que cumplan con los criterios especificados.\")\n",
    "                return\n",
    "\n",
    "            # Ordenar los cambios por magnitud\n",
    "            abrupt_changes.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "\n",
    "            # Mostrar los 10 cambios mÃ¡s grandes\n",
    "            num_changes_to_show = min(10, len(abrupt_changes))\n",
    "\n",
    "            # Mostrar los cambios mÃ¡s abruptos\n",
    "            max_cause_length = max(len(str(cause)) for cause, _, _, _, _, _ in abrupt_changes)\n",
    "            for cause, previous_period, current_period, change, previous_value, current_value in abrupt_changes[:num_changes_to_show]:\n",
    "                previous_period_str = f\"{previous_period}\" if isinstance(previous_period, int) else f\"{previous_period[0]}, {previous_period[1]}\"\n",
    "                current_period_str = f\"{current_period}\" if isinstance(current_period, int) else f\"{current_period[0]}, {current_period[1]}\"\n",
    "                cause_str = str(cause)\n",
    "                logger.warning(f\"Para '{cause_str:<{max_cause_length}}', el cambio mÃ¡s grande fue de {change:>8.2f}% (+{current_value - previous_value:>4}) pasando de {previous_value:>4} a {current_value:>4} entre el perÃ­odo {previous_period_str} a {current_period_str}.\")\n",
    "\n",
    "        \n",
    "        logger.info(f\"Config keys used: {index_columns}, {columns if columns else 'pivot_columns'}, number_column\")\n",
    "\n",
    "        if df is None:\n",
    "            df = self.df\n",
    "\n",
    "        index = [self.config[col] for col in index_columns]\n",
    "\n",
    "        if columns is None:\n",
    "            columns = self.config['pivot_columns']\n",
    "        else:\n",
    "            columns = [self.config[col] for col in columns]\n",
    "\n",
    "        pivot = (\n",
    "            df\n",
    "            .pivot_table(\n",
    "                index=index,\n",
    "                columns=columns,\n",
    "                values=self.config['number_column'],\n",
    "                aggfunc='count',\n",
    "                fill_value=0\n",
    "            )\n",
    "        )\n",
    "        multi_index = pivot.columns.nlevels > 1\n",
    "        if multi_index:\n",
    "            pivot = flatten_df_columns(pivot)\n",
    "\n",
    "        pivot['TOTAL'] = pivot.sum(axis=1)\n",
    "        # filter pivot TOTAL != 0\n",
    "        pivot = pivot[pivot['TOTAL'] != 0]\n",
    "        sort_cols = ['TOTAL'] + [col for col in pivot.columns if col != 'TOTAL']\n",
    "        top_recurring_cases = (\n",
    "            pivot\n",
    "            .sort_values(by=sort_cols, ascending=[False] + [False] * (len(sort_cols) - 1))\n",
    "            .loc[:, lambda x: (x != 0).any(axis=0)]\n",
    "        )\n",
    "        logger.info(f\"Recurring cases found in created tickets:\\nIndex columns: {index}, Len: {top_recurring_cases.shape[0]}\")\n",
    "        try:\n",
    "            logger.info(\"\\n\" + tabulate(top_recurring_cases.head(10), headers='keys', tablefmt='psql'))\n",
    "            changes(pivot)\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.info('\\n' + str(top_recurring_cases))\n",
    "\n",
    "    def analyze(self):\n",
    "        # Check for duplicates\n",
    "        self.check_duplicates(column_name='number_column')\n",
    "        self.check_duplicates(column_name='problem_column')\n",
    "        # Check for missing values\n",
    "        self.check_missing(column_name='number_column')\n",
    "        self.check_missing(column_name='problem_column')\n",
    "        # Check for priority problems\n",
    "        self.check_priority_problems(priority_list='high_priority_list')\n",
    "        self.check_priority_problems(priority_list='medium_priority_list', has_problem=False)\n",
    "        self.check_priority_problems(priority_list='low_priority_list', has_problem=False)\n",
    "\n",
    "        # Clean string columns\n",
    "        for col in self.config['string_columns']:\n",
    "            self.clean_str_column(col)\n",
    "\n",
    "        # Generate pivot tables\n",
    "        self.get_pivot(index_columns=['priority_column'], columns=['date_column'])\n",
    "        self.get_pivot(index_columns=['cause_code_column'])\n",
    "        self.get_pivot(index_columns=['close_code_column'])\n",
    "        self.get_pivot(index_columns=['cause_code_column', 'close_code_column'])\n",
    "\n",
    "        list_suffixes = self.config['suffixes']\n",
    "        suffix_pattern = r'(' + '|'.join(list_suffixes) + r')$'\n",
    "\n",
    "        self.get_pivot(\n",
    "            index_columns=['service_offering_clean_column'],\n",
    "            df=self.df.assign(SERVICE_OFFERING_clean=lambda x: x[self.config['service_offering_column']]\n",
    "                            .str.replace(r'^\\d+_', '', regex=True)\n",
    "                            .str.replace(suffix_pattern, '', regex=True))\n",
    "        )\n",
    "        \n",
    "        self.get_pivot(\n",
    "            index_columns=['service_offering_clean_column'],\n",
    "            df=self.df.assign(SERVICE_OFFERING_clean=lambda x: x[self.config['service_offering_column']]\n",
    "                            .str.replace(r'^\\d+_', '', regex=True)\n",
    "                            .str.replace(suffix_pattern, '', regex=True)\n",
    "                            .str.split('_').str[-1])\n",
    "        )\n",
    "        \n",
    "        self.get_pivot(\n",
    "            index_columns=['service_offering_clean_column'],\n",
    "            df=self.df.assign(SERVICE_OFFERING_clean=lambda x: x[self.config['service_offering_column']]\n",
    "                            .str.replace(r'^\\d+_', '', regex=True)\n",
    "                            .str.replace(suffix_pattern, '', regex=True)\n",
    "                            .str.split('_').str[0])\n",
    "        )\n",
    "\n",
    "        self.get_pivot(index_columns=['business_service_column'])\n",
    "        self.get_pivot(index_columns=['creator_group_column'])\n",
    "\n",
    "        for p in self.df[self.config['priority_column']].unique():\n",
    "            self.get_pivot(\n",
    "                index_columns=['assignment_group_column', 'filtro_creator_group_column', 'filtro_grupo_column'],\n",
    "                df=self.df.loc[lambda x: x[self.config['priority_column']] == p]\n",
    "            )\n",
    "\n",
    "        # display(self.df)\n",
    "\n",
    "# Configuration dictionary\n",
    "config = {\n",
    "    'number_column': 'NUMBER',\n",
    "    'problem_column': 'PROBLEM',\n",
    "    # 'problem_column': 'CREATED_OUT_OF_INCIDENT',\n",
    "    'priority_column': 'CURRENT_PRIORITY',\n",
    "    'date_column': '_CREATED_year',\n",
    "    # 'date_column': 'CREATED_year',\n",
    "    # 'cause_code_column': 'CAUSE_CODE',\n",
    "    'cause_code_column': 'STATE',\n",
    "    'close_code_column': 'CLOSE_CODE',\n",
    "    'service_offering_column': 'SERVICE_OFFERING',\n",
    "    'service_offering_clean_column': 'SERVICE_OFFERING_clean',\n",
    "    'business_service_column': 'BUSINESS_SERVICE',\n",
    "    # 'business_service_column': 'TASK_TYPE',\n",
    "    'creator_group_column': 'CREATOR_GROUP',\n",
    "    'assignment_group_column': 'ASSIGNMENT_GROUP',\n",
    "    # 'filtro_creator_group_column': 'FILTRO_CREATOR_GROUP',\n",
    "    'filtro_creator_group_column': 'PRIORITY',\n",
    "    # 'filtro_grupo_column': 'FILTRO_GRUPO',\n",
    "    'filtro_grupo_column': 'AFFECTED_OES',\n",
    "    'string_columns': ['CAUSE_CODE', 'CLOSE_CODE', 'SERVICE_OFFERING', 'BUSINESS_SERVICE', 'CREATOR_GROUP', 'ASSIGNMENT_GROUP'],\n",
    "    # 'string_columns': ['STATE', 'CLOSE_CODE', 'SERVICE_OFFERING', 'TASK_TYPE', 'CREATOR_GROUP', 'ASSIGNMENT_GROUP', 'PRIORITY', 'AFFECTED_OES'],\n",
    "    'pivot_columns': ['CURRENT_PRIORITY', '_CREATED_year'],\n",
    "    # 'pivot_columns': ['CURRENT_PRIORITY', 'CREATED_year'],\n",
    "    'high_priority_list': ['P2', 'P1'],\n",
    "    'medium_priority_list': ['P3'],\n",
    "    'low_priority_list': ['P4'],\n",
    "    'suffixes': ['_ESP', '_IBL', '_PT', '_BR', '_CO', '_ES', '_US', '_AZMEX', '_IBEROLATAM']\n",
    "}\n",
    "\n",
    "# Create an instance of IncidentAnalyzer\n",
    "analyzer = IncidentAnalyzer(df_incidents_grouped.pipe(create_dt_time_wvariants), config)\n",
    "print(df_incidents_grouped.columns)\n",
    "# Execute the analysis\n",
    "analyzer.analyze()\n",
    "\n",
    "# add column of %s and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cbbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# df = df_incidents.copy()\n",
    "# # Group by 'NUMBER' and calculate the number of unique values in 'RESOLVED' and 'CLOSED'\n",
    "# df_inter = df.groupby('NUMBER')[['RESOLVED', 'CLOSED']].nunique()\n",
    "\n",
    "# # Filter to find 'NUMBER' where either 'RESOLVED' or 'CLOSED' has more than 1 unique value\n",
    "# reopened_tickets = df_inter[(df_inter['RESOLVED'] > 1) | (df_inter['CLOSED'] > 1)].index.tolist()\n",
    "\n",
    "# # Sample 10 elements from the list of reopened tickets\n",
    "# sampled_reopened_tickets = random.sample(reopened_tickets, min(5, len(reopened_tickets)))\n",
    "\n",
    "# # print(\"Tickets that have been reopened:\", reopened_tickets)\n",
    "\n",
    "# cols = ['NUMBER', 'PROBLEM', 'CREATED', 'RESOLVED', 'CREATED_BY', 'PRIORITY', 'CURRENT_PRIORITY', 'CLOSED', 'UPDATED']\n",
    "# df[cols][df['NUMBER'].isin(sampled_reopened_tickets)].to_dict(orient=\"records\")\n",
    "\n",
    "# print(df.columns)\n",
    "# display(df)\n",
    "\n",
    "# test77_reopened_incidents_TREND(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_incidents_grouped.sample(5).to_dict(orient=\"records\")\n",
    "# display(df_incidents_grouped.head())\n",
    "# # Example usage\n",
    "# # Assuming df is your DataFrame\n",
    "# result = test01_problems_aging(df_incidents_grouped)\n",
    "# tabulate_df(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11021f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_incidents.loc[lambda x: x['NUMBER'] == 'INC27459004'].sort_values(by=['NUMBER','REASSIGNMENT_COUNT','UPDATED']).nunique().reset_index().set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cec29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfd167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = (pd.merge(df_incidents_grouped.add_suffix(\"_incident\"), \n",
    "                              df_problem.add_suffix(\"_problem\"), \n",
    "                              left_on=\"NUMBER_incident\", \n",
    "                              right_on=\"CREATED_OUT_OF_INCIDENT_problem\", \n",
    "                              suffixes=(\"_incident\", \"_problem\"), \n",
    "                              how=\"outer\", \n",
    "                              indicator=True, \n",
    "                              validate=\"m:m\").rename(columns={\"_merge\": \"_merge_inc_pro\"})\n",
    "                    #  [['NUMBER_incident','NUMBER_problem']].value_counts(sort=True)\n",
    "                    # .NUMBER_problem.value_counts(sort=True, dropna=False) # ok 1:m\n",
    "                    .merge(df_problem_tasks.add_suffix(\"_task\"), \n",
    "                           left_on=\"NUMBER_problem\", \n",
    "                           right_on=\"PROBLEM_task\", \n",
    "                           suffixes=(\"\", \"_task\"), \n",
    "                           how=\"outer\", \n",
    "                           indicator=True, \n",
    "                           validate=\"m:m\").rename(columns={\"_merge\": \"_merge_pro_task\"}) # m:m por nas\n",
    "                    #  ._merge_pro_task.value_counts()\n",
    "                    .sort_values(by=[\"CREATED_incident\",\"RESOLVED_incident\"])\n",
    "                    # .to_excel(intermediate_data+\"//merged_incidents_servicenow.xlsx\", index=False)\n",
    ")\n",
    "\n",
    "casos_audit[\"INC_PBR_PBT_merged\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (pd.merge(df_created_incidents, df_resolved_incidents, on='NUMBER', suffixes=('_created', '_resolved'), how='outer', indicator=True, validate=\"one_to_one\")\n",
    "#  .sort_values(by=[\"DATE_CREATED\", \"DATE_RESOLVED\", \"CLOSED\", \"CREATED\", \"RESOLVED\"])\n",
    "#  .to_excel(intermediate_data+\"//merged_incidents.xlsx\", index=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293de150",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_audit[\"INCIDENTS_ES\"] = df_incidents_grouped\n",
    "casos_audit[\"INCIDENTS_ES_rows\"] = df_incidents\n",
    "casos_audit[\"PROBLEMS_ES\"] = df_problem\n",
    "casos_audit[\"PROBLEMS_TASKS_ES\"] = df_problem_tasks\n",
    "\n",
    "casos_audit['COLS_df_incidents_grouped'] = analyze_dataframe_columns_export(df_incidents_grouped)\n",
    "casos_audit['COLS_df_problem'] = analyze_dataframe_columns_export(df_problem)\n",
    "casos_audit['COLS_df_problem_tasks'] = analyze_dataframe_columns_export(df_problem_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flatten_df(df):\n",
    "    \"\"\"Convierte columnas MultiIndex en nombres planos.\"\"\"\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df = df.copy()\n",
    "        df.columns = [\n",
    "            \"_\".join([str(c) for c in col if c]) for col in df.columns.values\n",
    "        ]\n",
    "    return df.reset_index()  # aseguramos que el Ã­ndice no sea jerÃ¡rquico\n",
    "\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl.styles import Font\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "def create_excel_with_links(df_dict_to_export, name_file, path=\".\"):\n",
    "    \"\"\"\n",
    "    Exporta un diccionario de DataFrames a un Excel con:\n",
    "      - Hoja inicial de resumen (filas, columnas, nombres de columnas)\n",
    "      - HipervÃ­nculos desde la hoja inicial a cada sheet\n",
    "      - Ajuste automÃ¡tico de anchos de columna\n",
    "    \"\"\"\n",
    "\n",
    "    output_path = os.path.join(path, name_file)\n",
    "\n",
    "    # Crear Excel temporal con todas las hojas\n",
    "    with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
    "        # Hoja inicial con metadatos\n",
    "        initial_df = pd.DataFrame({\n",
    "            \"Sheet Name\": list(df_dict_to_export.keys()),\n",
    "            \"Rows\": [df.shape[0] for df in df_dict_to_export.values()],\n",
    "            \"Columns\": [df.shape[1] for df in df_dict_to_export.values()],\n",
    "            \"Column Names\": [\n",
    "                \", \".join([str(col) for col in df.columns.tolist()])\n",
    "                for df in df_dict_to_export.values()\n",
    "            ]\n",
    "        })\n",
    "        initial_df.to_excel(writer, sheet_name=\"Initial\", index=False)\n",
    "\n",
    "        # Escribir cada DataFrame (aplanando si hace falta)\n",
    "        for sheet_name, df in df_dict_to_export.items():\n",
    "            df_flat = _flatten_df(df)\n",
    "            df_flat.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Reabrir con openpyxl para aplicar hipervÃ­nculos y estilos\n",
    "    workbook = openpyxl.load_workbook(output_path)\n",
    "    summary_sheet = workbook[\"Initial\"]\n",
    "\n",
    "    # HipervÃ­nculos en columna A\n",
    "    for row in range(2, len(df_dict_to_export) + 2):\n",
    "        cell = summary_sheet.cell(row=row, column=1)\n",
    "        sheet_name = cell.value\n",
    "        cell.hyperlink = f\"#{sheet_name}!A1\"\n",
    "        cell.font = Font(underline=\"single\", color=\"0563C1\")\n",
    "\n",
    "    # Ajuste de anchos\n",
    "    for col in range(1, summary_sheet.max_column + 1):\n",
    "        column_letter = get_column_letter(col)\n",
    "        max_length = max(\n",
    "            len(str(summary_sheet.cell(row=row, column=col).value))\n",
    "            for row in range(1, summary_sheet.max_row + 1)\n",
    "        )\n",
    "        summary_sheet.column_dimensions[column_letter].width = min(max_length + 3, 100)\n",
    "\n",
    "    # Guardar final\n",
    "    workbook.save(output_path)\n",
    "    print(f\"âœ… Excel con hipervÃ­nculos creado en: {output_path}\")\n",
    "\n",
    "# Specify the file name for the Excel file\n",
    "excel_file_name = 'DA_INCIDENT_PROBLEMS.xlsx'\n",
    "create_excel_with_links(casos_audit, excel_file_name, path=\".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
